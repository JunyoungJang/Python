{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Iterative Policy Evaluation, VI and PI\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Reinforcement Learning: 4 Model-Free Prediction [David Silver](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) [local-video](http://localhost:8888/notebooks/Dropbox/Video/RL Course by David Silver - Lecture 4_ Model-Free Prediction.mp4) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning by David Silver 4.pdf) [slide](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf)\n",
    "\n",
    "- Reinforcement Learning: 5 Model Free Control [David Silver](https://www.youtube.com/watch?v=0g4j2k_Ggc4&index=5&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning by David Silver 5.pdf) [slide](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf)\n",
    "\n",
    "- Tutorial: Deep Reinforcement Learning, ICML 2016 [David Silver](http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/deep_rl_tutorial.pdf)\n",
    "\n",
    "- Machine Learning, part III: The Q-learning algorithm [JAKE BENNETT](https://articles.wearepop.com/secret-formula-for-self-learning-computers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "||Algorithm|\n",
    "|---|---|\n",
    "|Policy evaluation|Iterative Policy Evaluation|\n",
    "|Policy improvement|Value Iteration|\n",
    "||Policy Iteration|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterative Policy Evaluation - $v_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $q_{\\pi}(s,a)=0$ and $v_{\\pi}(s)=0$ for all $s$ and $a$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ and $a$ (synchronous or asynchronous) update $q_\\pi$ and $v_\\pi$ using Bellman's expectation equation: \n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# bad policy in Andrew Ng's lecture 16\n",
    "\n",
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Downarrow$|H|$\\Rightarrow$|-1|\n",
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Uparrow$|$\\Uparrow$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# optimal policy in Andrew Ng's lecture 16\n",
    "\n",
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Uparrow$|H|$\\Uparrow$|-1|\n",
    "|$\\Uparrow$|$\\Leftarrow$|$\\Leftarrow$|$\\Leftarrow$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# value function\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1\n",
    "V[6] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        V[s] = sum([policy[s,a]*(R[s,a]+ gamma*sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)])) for a in range(N_ACTIONS)])\n",
    "    V[3] = 1\n",
    "    V[6] = -1\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71576205  0.74319399  0.772       1.          0.69132019  0.76103021\n",
      " -1.          0.66440699  0.64042733  0.61402305  0.60243653]\n"
     ]
    }
   ],
   "source": [
    "# iterative policy evaluation - v_\\pi\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# value function\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1\n",
    "V[6] = -1\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        V[s] = sum([policy[s,a]*(R[s,a]+ gamma*sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)])) for a in range(N_ACTIONS)])\n",
    "    V[3] = 1\n",
    "    V[6] = -1\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterative Policy Evaluation for $q_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $q_{\\pi}(s,a)=0$ and $v_{\\pi}(s)=0$ for all $s$ and $a$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ and $a$ (synchronous or asynchronous) update $q_\\pi$ and $v_\\pi$ using Bellman's expectation equation: \n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# value function\n",
    "# V = np.zeros(N_STATES)\n",
    "# V[3] = 1\n",
    "# V[6] = -1\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     for s in range(N_STATES):\n",
    "#         V[s] = sum([policy[s,a]*(R[s,a]+ gamma*sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)])) for a in range(N_ACTIONS)])\n",
    "#     V[3] = 1\n",
    "#     V[6] = -1\n",
    "# print(V)\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*sum([policy[s1,a1]*Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65632878  0.76103021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41678095  0.55808791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.84456801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# iterative policy evaluation - q_\\pi\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# value function\n",
    "# V = np.zeros(N_STATES)\n",
    "# V[3] = 1\n",
    "# V[6] = -1\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# for i in range(100):\n",
    "#     for s in range(N_STATES):\n",
    "#         V[s] = sum([policy[s,a]*(R[s,a]+ gamma*sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)])) for a in range(N_ACTIONS)])\n",
    "#     V[3] = 1\n",
    "#     V[6] = -1\n",
    "# print(V)\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*sum([policy[s1,a1]*Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value iteration\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $q_*(s,a)=0$ and $v_*(s)=0$ for all $s$ and $a$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ and $a$ (synchronous or asynchronous) update $q_*$ and $v_*$ using Bellman's optimality equation: \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\max_{a'}q_*(s',a')\\right)\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*max([Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65632878  0.76103021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41678095  0.55808791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.84456801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# value iteration - q_\\pi\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "for t in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*max([Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/Policy Iteration.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Initialize $\\pi$ randomly.\n",
    "\n",
    "- Repeat\n",
    "\n",
    "    Update $q_\\pi$ and $v_\\pi$ by solving Bellman's expectation equation.\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "    \n",
    "    Update $\\pi$ by solving\n",
    "\n",
    "$$\n",
    "\\pi(s)=\\mbox{argmax}_{a}q_\\pi(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "\n",
    "    # evaluate policy\n",
    "    for i in range(100):\n",
    "        for s in range(N_STATES):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*sum([policy[s1,a1]*Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "        Q[3,:] = 1\n",
    "        Q[6,:] = -1\n",
    "        #print(Q)\n",
    "    \n",
    "    # improve policy\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    m = np.argmax(Q,1)\n",
    "    for i in range(N_STATES):\n",
    "        policy[i,m[i]] = 1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65632878  0.76103021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41678095  0.55808791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.84456801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# policy iteration - q_\\pi\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "for t in range(100):\n",
    "\n",
    "    # evaluate policy\n",
    "    for i in range(100):\n",
    "        for s in range(N_STATES):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*sum([policy[s1,a1]*Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "        Q[3,:] = 1\n",
    "        Q[6,:] = -1\n",
    "        #print(Q)\n",
    "    \n",
    "    # improve policy\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    m = np.argmax(Q,1)\n",
    "    for i in range(N_STATES):\n",
    "        policy[i,m[i]] = 1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Asynchronous Dynamic Programming\n",
    "\n",
    "### In-Place Dynamic Programming\n",
    "\n",
    "### Prioritised Sweeping\n",
    "\n",
    "### Real-Time Dynamic Programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# In-Place Dynamic Programming\n",
    "\n",
    "<img src=\"img/In-Place Dynamic Programming.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Prioritised Sweeping\n",
    "\n",
    "<img src=\"img/Prioritised Sweeping.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Real-Time Dynamic Programming\n",
    "\n",
    "<img src=\"img/Real-Time Dynamic Programming.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
