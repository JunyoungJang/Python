{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model Learning\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Reinforcement Learning: 4 Model-Free Prediction [David Silver](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) [local-video](http://localhost:8888/notebooks/Dropbox/Video/RL Course by David Silver - Lecture 4_ Model-Free Prediction.mp4) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning by David Silver 4.pdf) [slide](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf)\n",
    "\n",
    "- Reinforcement Learning: 5 Model Free Control [David Silver](https://www.youtube.com/watch?v=0g4j2k_Ggc4&index=5&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning by David Silver 5.pdf) [slide](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf)\n",
    "\n",
    "- Tutorial: Deep Reinforcement Learning, ICML 2016 [David Silver](http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/deep_rl_tutorial.pdf)\n",
    "\n",
    "- Machine Learning, part III: The Q-learning algorithm [JAKE BENNETT](https://articles.wearepop.com/secret-formula-for-self-learning-computers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "||Algorithm|\n",
    "|---|---|\n",
    "|Policy evaluation|Iterative Policy Evaluation|\n",
    "|Policy improvement|Value Iteration|\n",
    "||Policy Iteration|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterative Policy Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Snrky_20130629_TC.jpg\" width=\"80%\"></div>\n",
    "\n",
    "http://4.bp.blogspot.com/-TGdTiKfQ23E/Uc79lskYtJI/AAAAAAAABj4/2ZuVOO3-8rI/s1600/Snrky_20130629_TC.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Iterative Policy Evaluation for $v_\\pi$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $v_{\\pi}(s)=0$ for all $s$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ (synchronous or asynchronous) update $v_\\pi$  using Bellman's expectation equation: \n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Iterative Policy Evaluation for $q_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $q_{\\pi}(s,a)=0$ for all $s$ and $a$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ and $a$ (synchronous or asynchronous) update $q_\\pi$ using Bellman's expectation equation: \n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterative Policy Evaluation for $v_\\pi$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-8.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-1.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71576205  0.74319399  0.772       1.          0.69132019  0.76103021\n",
      " -1.          0.66440699  0.64042733  0.61402305  0.60243653]\n"
     ]
    }
   ],
   "source": [
    "# Iterative Policy Evaluation for $v_\\pi$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# value function\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1\n",
    "V[6] = -1\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        V[s] = sum(\n",
    "                    [policy[s,a]*(R[s,a]+ gamma*\n",
    "                     sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)]))\n",
    "                     for a in range(N_ACTIONS)]\n",
    "                        )\n",
    "    V[3] = 1\n",
    "    V[6] = -1\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Find $v_\\pi$ for bad and random policy, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterative Policy Evaluation for $q_\\pi$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-9.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-2.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65632878  0.76103021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41678095  0.55808791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.84456801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# Iterative Policy Evaluation for $q_\\pi$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum(\n",
    "                                        [P[s,a,s1]*\n",
    "                                         sum([policy[s1,a1]*Q[s1,a1] for a1 in range(N_ACTIONS)])\n",
    "                                         for s1 in range(N_STATES)]\n",
    "                                            )\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Find $q_\\pi$ for bad and random policy, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Value.jpg\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "http://seanheritage.com/wp-content/uploads/2017/03/Value.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value iteration for $v_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $v_*(s)=0$ for all $s$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$  (synchronous or asynchronous) update $v_*$ using Bellman's optimality equation: \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "v_*(s)&=&\\max_{a}\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Find optimal policy $\\pi_*$ by solving\n",
    "\n",
    "$$\n",
    "\\pi_*(s)=\\mbox{argmax}_{a}q_*(s,a)\n",
    "$$\n",
    "\n",
    "    where\n",
    "$$\n",
    "q_*(s,a)={\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value iteration for $q_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $q_*(s,a)=0$ for all $s$ and $a$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ and $a$ (synchronous or asynchronous) update $q_*$ using Bellman's optimality equation: \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\max_{a'}q_*(s',a')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Find optimal policy $\\pi_*$ by solving\n",
    "\n",
    "$$\n",
    "\\pi_*(s)=\\mbox{argmax}_{a}q_*(s,a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value iteration for $v_*$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/cs188_mdp_optimal_policies.png\" width=\"70%\" height=\"10%\"></div>\n",
    "\n",
    "https://raw.githubusercontent.com/mebusy/notes/master/imgs/cs188_mdp_optimal_policies.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-7.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71576205  0.74319399  0.772       1.          0.69132019  0.76103021\n",
      " -1.          0.66440699  0.64042733  0.61402305  0.60243653]\n",
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65335878  0.76400021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41777095  0.55907791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.83565801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# Value iteration for $v_*$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0]\n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1]\n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))\n",
    "else: # fuel-inefficient robot\n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# value function\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1\n",
    "V[6] = -1\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        V[s] = max(\n",
    "                    [R[s,a] + gamma *\n",
    "                     sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)])\n",
    "                     for a in range(N_ACTIONS)]\n",
    "                        )\n",
    "    V[3] = 1\n",
    "    V[6] = -1\n",
    "print(V)\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)])\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value iteration for $q_*$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/cs188_mdp_optimal_policies.png\" width=\"70%\" height=\"10%\"></div>\n",
    "\n",
    "https://raw.githubusercontent.com/mebusy/notes/master/imgs/cs188_mdp_optimal_policies.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-3.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.25396202 -0.76157779 -1.25396202 -1.69267636]\n",
      " [-1.29783345 -0.26421999 -0.76157779 -0.76157779]\n",
      " [-0.76157779  0.292      -0.21092    -0.81852795]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [-1.69267636 -1.69267636 -1.20472359 -2.13656999]\n",
      " [-0.74391994 -1.37188536 -0.24638378 -1.44348477]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [-2.17574959 -1.78395358 -1.69267636 -2.17574959]\n",
      " [-2.12744227 -1.29692281 -1.78395358 -1.78395358]\n",
      " [-1.78395358 -1.80306822 -0.86703795 -1.35836757]\n",
      " [-1.29692281 -1.78395358 -1.85198199 -1.78395358]]\n"
     ]
    }
   ],
   "source": [
    "# Value iteration for $q_*$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if False: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "for t in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum(\n",
    "                                        [P[s,a,s1]*\n",
    "                                         max([Q[s1,a1] for a1 in range(N_ACTIONS)]) \n",
    "                                         for s1 in range(N_STATES)]\n",
    "                                            )\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/Policy Iteration.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Policy iteration using $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $\\pi$ randomly.\n",
    "\n",
    "- Repeat\n",
    "\n",
    "    [Policy evaluation] Evalate $v_\\pi$ by iterating Bellman's expectation equation.\n",
    "\\begin{eqnarray*}\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "    \n",
    "    [Policy improvement] Improve $\\pi$ by solving\n",
    "\n",
    "$$\n",
    "\\pi(s)=\\mbox{argmax}_{a}q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "    where\n",
    "$$\n",
    "q_\\pi(s,a)={\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Policy iteration using $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $\\pi$ randomly.\n",
    "\n",
    "- Repeat\n",
    "\n",
    "    [Policy evaluation] Evalate $q_\\pi$ by iterating Bellman's expectation equation.\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "    \n",
    "    [Policy improvement] Improve $\\pi$ by solving\n",
    "\n",
    "$$\n",
    "\\pi(s)=\\mbox{argmax}_{a}q_\\pi(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy iteration using $v$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/cs188_mdp_optimal_policies.png\" width=\"70%\" height=\"10%\"></div>\n",
    "\n",
    "https://raw.githubusercontent.com/mebusy/notes/master/imgs/cs188_mdp_optimal_policies.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-11.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-10.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Policy iteration using $v$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0]\n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1]\n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))\n",
    "else: # fuel-inefficient robot\n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif True: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# value function\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1\n",
    "V[6] = -1\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "for t in range(100):\n",
    "\n",
    "    # policy evaluation - v\n",
    "    for i in range(100):\n",
    "        for s in range(N_STATES):\n",
    "            V[s] = sum(\n",
    "                        [policy[s, a] * (R[s, a] + gamma *\n",
    "                         sum([P[s, a, s1] * V[s1] for s1 in range(N_STATES)]))\n",
    "                         for a in range(N_ACTIONS)]\n",
    "                            )\n",
    "        V[3] = 1\n",
    "        V[6] = -1\n",
    "\n",
    "    # policy evaluation - q\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s, a] = R[s, a] + gamma * sum([P[s, a, s1] * V[s1] for s1 in range(N_STATES)])\n",
    "        Q[3, :] = 1\n",
    "        Q[6, :] = -1\n",
    "\n",
    "    # policy improvement\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    m = np.argmax(Q, 1)\n",
    "    for i in range(N_STATES):\n",
    "        policy[i, m[i]] = 1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy iteration using $q$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/cs188_mdp_optimal_policies.png\" width=\"70%\" height=\"10%\"></div>\n",
    "\n",
    "https://raw.githubusercontent.com/mebusy/notes/master/imgs/cs188_mdp_optimal_policies.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-11.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2017-4.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]\n",
      " [-50. -50. -50. -50.]]\n"
     ]
    }
   ],
   "source": [
    "# Policy iteration using $q$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if False: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif True: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "for t in range(100):\n",
    "\n",
    "    # policy evaluation\n",
    "    for i in range(100):\n",
    "        for s in range(N_STATES):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Q[s, a] = R[s, a] + gamma * sum(\n",
    "                                                    [P[s, a, s1] * \n",
    "                                                     sum([policy[s1, a1] * Q[s1, a1] for a1 in range(N_ACTIONS)]) \n",
    "                                                     for s1 in range(N_STATES)]\n",
    "                                                        )\n",
    "    \n",
    "    # policy improvement\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    m = np.argmax(Q,1)\n",
    "    for i in range(N_STATES):\n",
    "        policy[i,m[i]] = 1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Asynchronous Dynamic Programming\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### In-Place Dynamic Programming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Prioritised Sweeping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Real-Time Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# In-Place Dynamic Programming\n",
    "\n",
    "<img src=\"img/In-Place Dynamic Programming.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Prioritised Sweeping\n",
    "\n",
    "<img src=\"img/Prioritised Sweeping.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Real-Time Dynamic Programming\n",
    "\n",
    "<img src=\"img/Real-Time Dynamic Programming.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Check whether the code I provide is synchronous or asynchronous.\n",
    "If the code is asynchronous, make is synchronous.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
