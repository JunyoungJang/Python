{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model Free Learning\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Reinforcement Learning: 4 Model-Free Prediction [David Silver](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) [local-video](http://localhost:8888/notebooks/Dropbox/Video/RL Course by David Silver - Lecture 4_ Model-Free Prediction.mp4) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning by David Silver 4.pdf) [slide](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf)\n",
    "\n",
    "- Reinforcement Learning: 5 Model Free Control [David Silver](https://www.youtube.com/watch?v=0g4j2k_Ggc4&index=5&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning by David Silver 5.pdf) [slide](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf)\n",
    "\n",
    "- Tutorial: Deep Reinforcement Learning, ICML 2016 [David Silver](http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/deep_rl_tutorial.pdf)\n",
    "\n",
    "- Machine Learning, part III: The Q-learning algorithm [JAKE BENNETT](https://articles.wearepop.com/secret-formula-for-self-learning-computers)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model vs Model-free\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{llllll}\n",
    "\\mbox{Model}&\\quad\\Rightarrow\\quad&\\mbox{Model-free}\\\\\n",
    "\\mbox{Based on $P_{ss'}^a$}&\\quad\\Rightarrow\\quad&\\mbox{Based on Samples}\\\\\n",
    "V&\\quad\\Rightarrow\\quad&Q\\\\\n",
    "\\mbox{Greedy}&\\quad\\Rightarrow\\quad&\\mbox{$\\varepsilon$-Greedy}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We use $Q$ instead of $V$ when we don't know the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Model\n",
    "\n",
    "If we know $R_s^a$, $P_{ss'}^a$, and $V$, and if we are at state $s$, our next action is\n",
    "$$\n",
    "\\mbox{argmax}_a Q(s,a) \\quad =\\quad \\mbox{argmax}_a\\left(R_s^a + \\gamma * \\sum_{s'} P_{ss'}^a * V(s')\\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Model-free\n",
    "\n",
    "In reality, typically we don't know $P_{ss'}^a$.\n",
    "So, we cannot decide our next action based on $V$.\n",
    "That is why we use $Q$, not $V$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We use $\\varepsilon$-greedy policy update instead of greedy policy update when we don't know the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# model\n",
    "\n",
    "If we know $R_s^a$, $P_{ss'}^a$, and $V$, every states $s$ are counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Model-free\n",
    "\n",
    "- In reality, typically we don't know $P_{ss'}^a$.\n",
    "\n",
    "- So, we take samples.\n",
    "\n",
    "- If we update policy greedily, we may miss good regions in state space.\n",
    "\n",
    "- That is why we use $\\varepsilon$-greedy policy update instead of greedy policy update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On and Off-Policy Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### On-policy learning\n",
    "\n",
    "- “Learn on the job”\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\pi$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Off-policy learning\n",
    "\n",
    "- “Look over someone’s shoulder”\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Sample $V$|Sample $Q$|Sample $Q$ (off-policy)|\n",
    "|---|---|\n",
    "|MC|MC|\n",
    "|TD|SARSA|Q-learnig|\n",
    "|TD($\\lambda$)|SARSA($\\lambda$)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to learn $v_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Monte-Carlo (MC)  Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Temporal-Difference (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Monte-Carlo (MC)  Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### First-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "Update $\\color{red}{\\mbox{first}}$ time-step $t$ that state $s$ is visited in an episode:\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "\\mbox{Increment counter}&&N(s)\\quad\\leftarrow\\quad N(s)+1\\\\\n",
    "\\mbox{Increment total return }&&S(s)\\quad\\leftarrow\\quad S(s)+G_t\\\\\n",
    "\\mbox{Value is estimated by mean return }&&V(s)\\quad\\leftarrow\\quad S(s)/N(s)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Every-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "Update $\\color{red}{\\mbox{every}}$  time-step $t$ that state $s$ is visited in an episode:\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "\\mbox{Increment counter}&&N(s)\\quad\\leftarrow\\quad N(s)+1\\\\\n",
    "\\mbox{Increment total return }&&S(s)\\quad\\leftarrow\\quad S(s)+G_t\\\\\n",
    "\\mbox{Value is estimated by mean return }&&V(s)\\quad\\leftarrow\\quad S(s)/N(s)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Incremental Mean\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mu_k\n",
    "&=&\\frac{1}{k}\\sum_{j=1}^kx_j\\nonumber\\\\\n",
    "&=&\\frac{1}{k}\\left(x_k+\\sum_{j=1}^{k-1}x_j\\right)\\nonumber\\\\\n",
    "&=&\\frac{1}{k}\\left(x_k+(k-1)\\mu_{k-1}\\right)\\nonumber\\\\\n",
    "&=&\\mu_{k-1}+\\frac{1}{k}\\left(x_k-\\mu_{k-1}\\right)\\nonumber\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Incremental Monte-Carlo (MC)  Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### First-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "Update $\\color{red}{\\mbox{first}}$ time-step $t$ that state $s$ is visited in an episode:\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "\\mbox{Increment counter}&&N(s)\\quad\\leftarrow\\quad N(s)+1\\\\\n",
    "\\mbox{Value is estimated by mean return }&&V(s)\\quad\\leftarrow\\quad V(s)+\\frac{1}{N(s)}(G_t-V(s))\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Every-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "Update $\\color{red}{\\mbox{every}}$  time-step $t$ that state $s$ is visited in an episode:\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "\\mbox{Increment counter}&&N(s)\\quad\\leftarrow\\quad N(s)+1\\\\\n",
    "\\mbox{Value is estimated by mean return }&&V(s)\\quad\\leftarrow\\quad V(s)+\\frac{1}{N(s)}(G_t-V(s))\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-stationary Incremental Monte-Carlo (MC)  Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### First-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "Update $\\color{red}{\\mbox{first}}$ time-step $t$ that state $s$ is visited in an episode:\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "\\mbox{Increment counter}&&N(s)\\quad\\leftarrow\\quad N(s)+1\\\\\n",
    "\\mbox{Value is estimated by mean return }&&V(s)\\quad\\leftarrow\\quad V(s)+\\alpha(G_t-V(s))\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Every-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "Update $\\color{red}{\\mbox{every}}$  time-step $t$ that state $s$ is visited in an episode:\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "\\mbox{Increment counter}&&N(s)\\quad\\leftarrow\\quad N(s)+1\\\\\n",
    "\\mbox{Value is estimated by mean return }&&V(s)\\quad\\leftarrow\\quad V(s)+\\alpha(G_t-V(s))\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temporal-Difference (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### MC\n",
    "$$\n",
    "V(S_t)\\quad\\leftarrow\\quad V(S_t)+\\alpha(\\color{red}{G_t}-V(S_t))\n",
    "$$\n",
    "\n",
    "- Use actual return $G_t$\n",
    "\n",
    "- Have to wait the full episode finishes to get $G_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### TD\n",
    "$$\n",
    "V(S_t)\\quad\\leftarrow\\quad V(S_t)+\\alpha(\\color{red}{R_{t+1}+\\gamma V(S_{t+1})}-V(S_t))\n",
    "$$\n",
    "\n",
    "- Use estimated return $R_{t+1}+\\gamma V(S_{t+1})$\n",
    "\n",
    "- No need to wait\n",
    "\n",
    "- $R_{t+1}+\\gamma V(S_{t+1})$ is called TD target \n",
    "\n",
    "- $\\delta_t=R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$ is called TD error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias/Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Bias\n",
    "\n",
    "- Return $G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{T-(t+1)}R_T$ is unbiased estimate of $v_\\pi(S_t)$\n",
    "\n",
    "- True TD target $R_{t+1} + \\gamma v_\\pi (S_{t+1})$ is unbiased estimate of $v_\\pi(S_t)$\n",
    "\n",
    "- TD target $R_{t+1} + \\gamma V(S_{t+1})$ is biased estimate of $v_\\pi(S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Variance Trade-Off\n",
    "\n",
    "TD target is much lower variance than the return:\n",
    "\n",
    "- Return depends on many random actions, transitions, rewards\n",
    "\n",
    "- TD target depends on one random action, transition, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### TD\n",
    "$$\n",
    "V(S_t)\\quad\\leftarrow\\quad V(S_t)+\\alpha\\delta_t \n",
    "$$\n",
    "\n",
    "- for each step TD error $\\delta_t=R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$ update only one $V(s)$, i.e., $V(S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### TD($\\lambda$)\n",
    "$$\\begin{array}{lll}\n",
    "E_0(s)&=&0\\\\\n",
    "E_t(s)&=&\\gamma\\lambda E_{t-1}(s)+1(S_t=s)\\\\\n",
    "V(s)&\\leftarrow&V(s)+\\alpha\\delta_t \\color{red}{E_t(s)}\n",
    "\\end{array}$$\n",
    "\n",
    "- $E_t(s)$ is called Eligibility Trace\n",
    "\n",
    "- for each step TD error $\\delta_t=R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$ update all $V(s)$ using eligibility trace $E_t(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to learn $Q_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### SARSA (On-Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### SARSA($\\lambda$) (On-Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Q_learning (Off-Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SARSA \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### $\\varepsilon$-greedy policy update\n",
    "\n",
    "- Update $\\varepsilon$-greedy policy using udated $Q$\n",
    "$$\n",
    "\\pi(a|s)=\\left\\{\\begin{array}{ll}\n",
    "\\frac{\\varepsilon}{m}+(1-\\varepsilon)&\\mbox{if}\\ a=\\mbox{argmax}_{a'}Q(s,a')\\\\\n",
    "\\frac{\\varepsilon}{m}&\\mbox{otherwise}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "where $m$ is the number of actions.\n",
    "\n",
    "- Choose action using updated $\\varepsilon$-greedy policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### SARSA (On-Policy) update\n",
    "\n",
    "With $a_{t+1}$ from the data\n",
    "$$\n",
    "Q(s_t,a_t)\\quad\\leftarrow\\quad\n",
    "Q(s_t,a_t)+\\alpha(\\color{red}{r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})}-Q(s_t,a_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/RZBt6.png\" width=\"100%\" height=\"30%\"></div>\n",
    "\n",
    "https://i.stack.imgur.com/RZBt6.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 40000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_minus_coin) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_minus_coin) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-20 at 12.52.37 PM.png\" width=\"100%\" height=\"30%\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/SARSA result.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64172601  0.72565731  0.63725051  0.62982205]\n",
      " [ 0.64420366  0.7567999   0.66343141  0.6594926 ]\n",
      " [ 0.66424266  0.8045765   0.66301671  0.56554162]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.6229201   0.62211272  0.68768342  0.61019115]\n",
      " [ 0.71760822 -0.59018659  0.76680592  0.56852539]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.60369664  0.59191724  0.64438461  0.60234898]\n",
      " [ 0.60921321  0.58235457  0.58743109  0.58847096]\n",
      " [ 0.57669922  0.40626445  0.53731761  0.56571982]\n",
      " [ 0.5668107   0.55486233 -0.8220727   0.55710637]]\n"
     ]
    }
   ],
   "source": [
    "# SARSA\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 40000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_minus_coin) if i>0 ][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_minus_coin) if i>0 ][0]\n",
    "\n",
    "# SARSA\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(\n",
    "            transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # choose action using udated epsilon-greedy policy \n",
    "        if np.random.random(1) < epsilon:\n",
    "            a1 = random.choice(actions) \n",
    "        else:\n",
    "            a1 = np.argmax(Q[s1,:])\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Convergence of SARSA - GLIE and Robins-Monro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"img/GLIE.png\" width=\"100%\" height=\"30%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Convergence of Sarsa.png\" width=\"100%\" height=\"30%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "- In the SARSA code $\\varepsilon$ stays as a constant during the $t$ iteration.\n",
    "Modify the code and decrease $\\varepsilon$ values as $t$ increases.\n",
    "More specifically, as the theory sugests, modify the code so that\n",
    "$$\\varepsilon=\\varepsilon(t)\\sim\\frac{1}{t}$$\n",
    "\n",
    "- In the SARSA code $\\alpha$ stays as a constant during the $t$ iteration.\n",
    "Modify the code and decrease $\\alpha$ values as $t$ increases.\n",
    "More specifically, as the theory sugests, modify the code so that\n",
    "$$\\alpha=\\alpha(t)\\sim\\frac{1}{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SARSA($\\lambda$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### SARSA update\n",
    "\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t)\\quad\\leftarrow\\quad\n",
    "Q(s_t,a_t)+\\alpha\\delta_t\n",
    "$$\n",
    "\n",
    "- for each step SARSA error $\\delta_t=r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})$ update only one $Q(s,a)$, i.e., $Q(s_t,a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### SARSA($\\lambda$) update\n",
    "\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "E_0(s,a)&=&0\\\\\n",
    "E_t(s,a)&=&\\gamma\\lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)\\\\\n",
    "Q(s,a)&\\leftarrow&Q(s,a)+\\alpha\\delta_t \\color{red}{E_t(s,a)}\n",
    "\\end{array}$$\n",
    "\n",
    "- $E_t(s,a)$ is called Eligibility Trace\n",
    "\n",
    "- for each step SARSA error $\\delta_t=r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})$ update all $Q(s,a)$ using eligibility trace $E_t(s,a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "- Change the SARSA code to SARSA($\\lambda$).\n",
    "\n",
    "- Change the SARSA code to TD.\n",
    "\n",
    "- Change the SARSA code to TD($\\lambda$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-learning \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### $\\varepsilon$-greedy policy update\n",
    "\n",
    "- Update $\\varepsilon$-greedy policy using udated $Q$\n",
    "$$\n",
    "\\pi(a|s)=\\left\\{\\begin{array}{ll}\n",
    "\\frac{\\varepsilon}{m}+(1-\\varepsilon)&\\mbox{if}\\ a=\\mbox{argmax}_{a'}Q(s,a')\\\\\n",
    "\\frac{\\varepsilon}{m}&\\mbox{otherwise}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "where $m$ is the number of actions.\n",
    "\n",
    "- Choose action using updated $\\varepsilon$-greedy policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Q_learning  (Off-Policy) update\n",
    "\n",
    "\n",
    "- With a sampling $a'$ from the policy of interest, not from the data or the data generating policy\n",
    "$$\n",
    "Q(s_t,a_t)\\quad\\leftarrow\\quad\n",
    "Q(s_t,a_t)+\\alpha(\\color{red}{r_{t+1}+\\gamma Q(s_{t+1},a')}-Q(s_t,a_t))\n",
    "$$\n",
    "\n",
    "- If the policy of interest is greedy,\n",
    "$$\n",
    "Q(s_t,a_t)\\quad\\leftarrow\\quad\n",
    "Q(s_t,a_t)+\\alpha(\\color{red}{r_{t+1}+\\gamma \\max_{a'}Q(s_{t+1},a')}-Q(s_t,a_t))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"img/Images_Algorithm_pt2_3.gif\"/>\n",
    "\n",
    "https://articles.wearepop.com/secret-formula-for-self-learning-computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/JvJqR.png\" width=\"100%\" height=\"30%\"></div>\n",
    "\n",
    "https://i.stack.imgur.com/JvJqR.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-20 at 12.58.30 PM.png\" width=\"100%\" height=\"30%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Q-learning result.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67544707  0.67803994  0.67608312  0.67512419]\n",
      " [ 0.68461082  0.70556283  0.68539185  0.68604285]\n",
      " [ 0.68162438  0.74172205  0.68375186  0.60272364]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.67059329  0.67036965  0.67065487  0.64565962]\n",
      " [ 0.7486363  -0.64730674  0.75522853  0.59772213]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.64949565  0.62340071  0.67071652  0.64555185]\n",
      " [ 0.66125157  0.60904048  0.62317608  0.62640485]\n",
      " [ 0.63985155  0.39527106  0.54003685  0.59753791]\n",
      " [ 0.62310997  0.57822928 -0.87344868  0.58047171]]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 40000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_policy_now_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_prob_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# Q-learning\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(\n",
    "            transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # choose action using udated epsilon-greedy policy \n",
    "        if np.random.random(1) < epsilon:\n",
    "            a1 = random.choice(actions) \n",
    "        else:\n",
    "            a1 = np.argmax(Q[s1,:])\n",
    "        \n",
    "        # SARSA\n",
    "        # Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # Q-learning\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * max(Q[s1,:]) - Q[s,a])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN\n",
    "\n",
    "DQN paper: https://www.nature.com/articles/nature14236\n",
    "\n",
    "DQN source code: https://sites.google.com/a/deepmind.com/dqn/\n",
    "\n",
    "<div align=\"center\"><img src=\"img/DQN Nature.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Deep Reinforcement Learning in Atari.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/breakout.gif\" width=\"30%\" height=\"30%\"></div>\n",
    "\n",
    "http://ikuz.eu/wp-content/uploads/2015/02/breakout.gif\n",
    "\n",
    "DQN Breakout [DeepMind](https://www.youtube.com/watch?v=TmPfTpjtdgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/DQN in Atari.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/DQN Results in Atari.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Experience Replay in Deep Q-Networks.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why DQN works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Going from a single-layer network to a multi-layer convolutional network to approximate the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Utilizing a second “target” network, which we will use to compute target Q-values during our updates.\n",
    "\n",
    "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preview of DQN - Q-learning using experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/output_ahug9u_by_elphin_zephyr-daxvvvu.gif\"/>\n",
    "\n",
    "https://orig00.deviantart.net/1b54/f/2017/035/9/b/output_ahug9u_by_elphin_zephyr-daxvvvu.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 20000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES)) \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/WW1-Great-War-Cartoons-Punch-Magazine-Raven-Hill-1917-12-19-421.jpg\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "\n",
    "https://ssl.c.photoshelter.com/img-get/I0000x4Qkv5Ut3mo/s/900/720/WW1-Great-War-Cartoons-Punch-Magazine-Raven-Hill-1917-12-19-421.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make a memory for a deque of maxlen 100 for experience replay\n",
    "replay_meomory = deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make a deque of maxlen 100 for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Q-learning using experience replay\n",
    "for t in range(epoch_q_learning):\n",
    "    \n",
    "    # code skipped ###\n",
    "    \n",
    "    while not done: \n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # Q-learning using experience replay \n",
    "        # choose 7 experiences from the deque\n",
    "        sample = random.sample(replay_meomory, 7)\n",
    "        for i in range(7):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            # Q-learning\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Q-learning using experience replay.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Preview of DQN - Q-learning using experience replay\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 20000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen 100 for experience replay\n",
    "replay_meomory = deque(maxlen=100)\n",
    "\n",
    "# make a deque of maxlen 100 for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n",
    "\n",
    "# Q-learning using experience replay\n",
    "for t in range(epoch_q_learning):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:]) \n",
    "    \n",
    "    while not done: \n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # Q-learning using experience replay \n",
    "        # choose 7 experiences from the deque\n",
    "        sample = random.sample(replay_meomory, 7)\n",
    "        for i in range(7):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            # Q-learning\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preview of DQN - Q-learning using experience replay and target Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/i2.cdn_.turner.commoneydamassets150930100632-target-bullseye-stunning-stats-1024x576-b25e28f4bec7a1d3ee6d3d9482367677e022c71e.jpg\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "https://www.markettamer.com/blog/wp-content/uploads/2017/10/i2.cdn_.turner.commoneydamassets150930100632-target-bullseye-stunning-stats-1024x576-b25e28f4bec7a1d3ee6d3d9482367677e022c71e.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 40000\n",
    "size_experience_replay = 1000\n",
    "number_of_sample_from_experience_replay = 20\n",
    "time_period_to_update_target_Q = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q \n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES)) \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen size_experience_replay for experience replay\n",
    "replay_meomory = deque(maxlen=size_experience_replay)\n",
    "\n",
    "# make a deque of maxlen size_experience_replay for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize target Q\n",
    "Q_target = deepcopy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Q-learning using experience replay and target Q\n",
    "for t in range(epoch_q_learning):\n",
    "\n",
    "    # code skipped ###\n",
    "    \n",
    "    # time log to update target Q\n",
    "    time_log_to_update_target_Q = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # time log to update target Q\n",
    "        time_log_to_update_target_Q += 1\n",
    "        \n",
    "        # code skipped ###\n",
    "\n",
    "        # Q-learning using experience replay and target Q\n",
    "        # choose number_of_sample_from_experience_replay experiences from the deque\n",
    "        sample = random.sample(replay_meomory, number_of_sample_from_experience_replay)\n",
    "        for i in range(number_of_sample_from_experience_replay):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            \n",
    "            # Q-learning\n",
    "            # Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "            #                      alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "            # Q-learning with target Q\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q_target[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "        # target Q update\n",
    "        if time_log_to_update_target_Q % time_period_to_update_target_Q == 0:\n",
    "            Q_target = deepcopy(Q)\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Q-learning using experience replay and target Q result.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preview of DQN - Q-learning using experience replay and target Q\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 40000\n",
    "size_experience_replay = 1000\n",
    "number_of_sample_from_experience_replay = 20\n",
    "time_period_to_update_target_Q = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q \n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen size_experience_replay for experience replay\n",
    "replay_meomory = deque(maxlen=size_experience_replay)\n",
    "\n",
    "# make a deque of maxlen size_experience_replay for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n",
    "\n",
    "\n",
    "# initialize target Q\n",
    "Q_target = deepcopy(Q)\n",
    "\n",
    "# Q-learning using experience replay and target Q\n",
    "for t in range(epoch_q_learning):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:]) \n",
    "    \n",
    "    # time log to update target Q\n",
    "    time_log_to_update_target_Q = 0\n",
    "    \n",
    "    while not done:\n",
    "        # exploit - update Q-function using Q-learning with experience replay\n",
    "        # and\n",
    "        # explore - move according to updated epsilon-greedy policy\n",
    "        \n",
    "        # time log to update target Q\n",
    "        time_log_to_update_target_Q += 1\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # Q-learning using experience replay and target Q\n",
    "        # choose number_of_sample_from_experience_replay experiences from the deque\n",
    "        sample = random.sample(replay_meomory, number_of_sample_from_experience_replay)\n",
    "        for i in range(number_of_sample_from_experience_replay):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            \n",
    "            # Q-learning\n",
    "            # Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "            #                      alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "            # Q-learning with target Q\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q_target[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "        # target Q update\n",
    "        if time_log_to_update_target_Q % time_period_to_update_target_Q == 0:\n",
    "            Q_target = deepcopy(Q)\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "I change just one line from the above code as below. Find the bug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "        # target Q update\n",
    "        if time_log_to_update_target_Q % time_period_to_update_target_Q == 0:\n",
    "            # Q_target = deepcopy(Q) # original line\n",
    "            Q_target = Q             # changed line"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
