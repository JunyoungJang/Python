{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Large-Scale Reinforcement Learning\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Reinforcement Learning: 6 Value Function Approximation [David Silver](https://www.youtube.com/watch?v=UoPei5o4fps&index=6&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning by David Silver 6.pdf) [slide](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf)\n",
    "\n",
    "- Tutorial: Deep Reinforcement Learning, ICML 2016 [David Silver](http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/deep_rl_tutorial.pdf)\n",
    "\n",
    "- Gradient Temporal Difference Networks [David Silver](http://proceedings.mlr.press/v24/silver12a/silver12a.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/silver12a.pdf) \n",
    "\n",
    "- Gradient Temporal-Difference Learning Algorithms [Hamid Maei](http://ai2-s2-pdfs.s3.amazonaws.com/9494/639c7d4cc4aad0842c18a26562903ca0c6f8.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/639c7d4cc4aad0842c18a26562903ca0c6f8.pdf) \n",
    "\n",
    "- Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond [Arthur Juliani](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df)\n",
    "\n",
    "- DQN [Lee Young Moo](http://www.phrgcm.com/blog/2016/08/17/deep-q-network/)\n",
    "\n",
    "- [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783v2.pdf)\n",
    "\n",
    "- [PR-005: Playing Atari with Deep Reinforcement Learning (NIPS 2013 Deep Learning Workshop)](https://www.youtube.com/watch?v=V7_cNTfm2i8&t=4s&list=PLlMkM4tgfjnJhhd4wn5aj8fVTYJwIpWkS&index=6)\n",
    "\n",
    "- DQN [nalsil](https://github.com/nalsil/TensorFlow-Tutorials/tree/master/07%20-%20DQN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Large-Scale Reinforcement Learning\n",
    "\n",
    "<div align=\"center\"><img src=\"img/Large-Scale Reinforcement Learning.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Function Approximation\n",
    "\n",
    "<div align=\"center\"><img src=\"img/Value Function Approximation.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Types of Value Function Approximation\n",
    "\n",
    "<div align=\"center\"><img src=\"img/Types of Value Function Approximation.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value Function Approximation By Stochastic Gradient Descent - Cheating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Goal\n",
    "\n",
    "Find ${\\bf w}$ minimizing\n",
    "$$\n",
    "J({\\bf w})=\\mathbb{E}_\\pi\\left(v_\\pi(S)-v_{\\bf w}(S)\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\mbox{Gradient descent}&\n",
    "\\Delta{\\bf w}\n",
    "=\\alpha\\mathbb{E}_\\pi\\left(v_\\pi(S)-v_{\\bf w}(S)\\right)\\nabla_{\\bf w}v_{\\bf w}(S)\\\\\n",
    "\\mbox{Stochastic gradient descent}&\n",
    "\\Delta{\\bf w}\n",
    "=\\alpha\\left(v_\\pi(S)-v_{\\bf w}(S)\\right)\\nabla_{\\bf w}v_{\\bf w}(S)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value Function Approximation By Incremental Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Goal\n",
    "\n",
    "Find ${\\bf w}$ minimizing\n",
    "$$\n",
    "J({\\bf w})=\\mathbb{E}_\\pi\\left(v_\\pi(S)-v_{\\bf w}(S)\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{llllll}\n",
    "\\mbox{MC}&\\Delta{\\bf w}&=&\\alpha&\\left(G_t-v_{\\bf w}(S)\\right)&\\nabla_{\\bf w}v_{\\bf w}(S)\\\\\n",
    "\\mbox{TD}&\\Delta{\\bf w}&=&\\alpha&\\left(R_{t+1}+\\gamma v_{\\bf w}(S_{t+1}) -v_{\\bf w}(S)\\right)&\\nabla_{\\bf w}v_{\\bf w}(S)\\\\\n",
    "\\mbox{TD}(\\lambda)&\\Delta{\\bf w}&=&\\alpha&\\left(G_t^{\\lambda}-v_{\\bf w}(S)\\right)&\\nabla_{\\bf w}v_{\\bf w}(S)\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN\n",
    "\n",
    "DQN paper: https://www.nature.com/articles/nature14236\n",
    "\n",
    "DQN source code: https://sites.google.com/a/deepmind.com/dqn/\n",
    "\n",
    "<div align=\"center\"><img src=\"img/DQN Nature.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Deep Reinforcement Learning in Atari.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/DQN in Atari.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/DQN Results in Atari.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why DQN works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Going from a single-layer network to a multi-layer convolutional network to approximate the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Utilizing a second “target” network, which we will use to compute target Q-values during our updates.\n",
    "\n",
    "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-learning using experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/output_ahug9u_by_elphin_zephyr-daxvvvu.gif\"/>\n",
    "\n",
    "https://orig00.deviantart.net/1b54/f/2017/035/9/b/output_ahug9u_by_elphin_zephyr-daxvvvu.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 20000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES)) \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"img/WW1-Great-War-Cartoons-Punch-Magazine-Raven-Hill-1917-12-19-421.jpg\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "\n",
    "https://ssl.c.photoshelter.com/img-get/I0000x4Qkv5Ut3mo/s/900/720/WW1-Great-War-Cartoons-Punch-Magazine-Raven-Hill-1917-12-19-421.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make a memory for a deque of maxlen 100 for experience replay\n",
    "replay_meomory = deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make a deque of maxlen 100 for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Q-learning using experience replay\n",
    "for t in range(epoch_q_learning):\n",
    "    \n",
    "    ... code skipped\n",
    "    \n",
    "    while not done: \n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # Q-learning using experience replay \n",
    "        # choose 7 experiences from the deque\n",
    "        sample = random.sample(replay_meomory, 7)\n",
    "        for i in range(7):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            # Q-learning\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Q-learning using experience replay.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05498236  0.54533103  0.07056754  0.03224705]\n",
      " [ 0.07911819  0.70386938  0.14433013  0.13376335]\n",
      " [ 0.1112421   0.79432344  0.2396675   0.08519836]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.02083935  0.01809598  0.3324215  -0.00364398]\n",
      " [ 0.15757644 -0.20682696  0.73833763  0.02644631]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [-0.00855757  0.01510166  0.08693695 -0.00556295]\n",
      " [-0.00731839  0.21470185 -0.00184952  0.00212867]\n",
      " [ 0.0079362  -0.01800231  0.42161678  0.05253249]\n",
      " [ 0.19009332  0.01269543 -0.23225642  0.00846619]]\n",
      "[[ 0.4585549   0.76560936  0.45227328  0.42416659]\n",
      " [ 0.44748585  0.78950609  0.49530373  0.51888518]\n",
      " [ 0.49591137  0.75993222  0.62025129  0.34950453]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.39306242  0.39920305  0.732524    0.38897126]\n",
      " [ 0.54592265 -0.70593815  0.81264371  0.33907882]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.35676667  0.3080031   0.67916175  0.34497249]\n",
      " [ 0.55154856  0.37293773  0.28650671  0.29991655]\n",
      " [ 0.37714219  0.24254151  0.22560698  0.28822026]\n",
      " [ 0.36824605  0.34027213 -0.93933829  0.33842648]]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning using experience replay\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 20000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen 100 for experience replay\n",
    "replay_meomory = deque(maxlen=100)\n",
    "\n",
    "# make a deque of maxlen 100 for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n",
    "\n",
    "# Q-learning using experience replay\n",
    "for t in range(epoch_q_learning):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:]) \n",
    "    \n",
    "    while not done: \n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # Q-learning using experience replay \n",
    "        # choose 7 experiences from the deque\n",
    "        sample = random.sample(replay_meomory, 7)\n",
    "        for i in range(7):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            # Q-learning\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-learning using experience replay and target Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/i2.cdn_.turner.commoneydamassets150930100632-target-bullseye-stunning-stats-1024x576-b25e28f4bec7a1d3ee6d3d9482367677e022c71e.jpg\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "https://www.markettamer.com/blog/wp-content/uploads/2017/10/i2.cdn_.turner.commoneydamassets150930100632-target-bullseye-stunning-stats-1024x576-b25e28f4bec7a1d3ee6d3d9482367677e022c71e.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 40000\n",
    "size_experience_replay = 1000\n",
    "number_of_sample_from_experience_replay = 20\n",
    "time_period_to_update_target_Q = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q \n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES)) \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen size_experience_replay for experience replay\n",
    "replay_meomory = deque(maxlen=size_experience_replay)\n",
    "\n",
    "# make a deque of maxlen size_experience_replay for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize target Q\n",
    "Q_target = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Q-learning using experience replay and target Q\n",
    "for t in range(epoch_q_learning):\n",
    "\n",
    "    ... code skipped \n",
    "    \n",
    "    # time log to update target Q\n",
    "    time_log_to_update_target_Q = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # time log to update target Q\n",
    "        time_log_to_update_target_Q += 1\n",
    "        \n",
    "        ... code skipped\n",
    "\n",
    "        # Q-learning using experience replay and target Q\n",
    "        # choose number_of_sample_from_experience_replay experiences from the deque\n",
    "        sample = random.sample(replay_meomory, number_of_sample_from_experience_replay)\n",
    "        for i in range(number_of_sample_from_experience_replay):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            \n",
    "            # Q-learning\n",
    "            # Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "            #                      alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "            # Q-learning with target Q\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q_target[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "        # target Q update\n",
    "        if time_log_to_update_target_Q % time_period_to_update_target_Q == 0:\n",
    "            Q_target = Q\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"img/Q-learning using experience replay and target Q result.png\" width=\"60%\" height=\"20%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.20477115e-02   6.23569214e-01   7.51497043e-02   3.49270849e-02]\n",
      " [  6.69469273e-02   7.34768345e-01   1.62949182e-01   1.33353783e-01]\n",
      " [  1.66233815e-01   7.53564952e-01   1.51510595e-01   8.99148776e-02]\n",
      " [  1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00]\n",
      " [  4.44500336e-02   2.25966833e-02   4.15697612e-01   5.49873502e-03]\n",
      " [  1.57705936e-01  -1.74401997e-01   7.50917635e-01   4.97867570e-02]\n",
      " [ -1.00000000e+00  -1.00000000e+00  -1.00000000e+00  -1.00000000e+00]\n",
      " [  6.36393639e-04  -3.39228410e-03   1.55186056e-01  -2.93230692e-03]\n",
      " [ -3.87006439e-03   2.01921509e-01  -5.42407939e-03  -1.78645296e-03]\n",
      " [ -1.30538920e-02  -2.62087774e-02   4.35722999e-01   9.18807343e-03]\n",
      " [  1.46041357e-01  -2.72626419e-03  -1.50432675e-01  -2.07415314e-03]]\n",
      "[[ 0.7269991   0.75107371  0.73514853  0.70416665]\n",
      " [ 0.72884115  0.78054021  0.76281138  0.76344136]\n",
      " [ 0.77573499  0.81651291  0.77941966  0.42552569]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.69511574  0.69957325  0.73405508  0.67891531]\n",
      " [ 0.77540263 -0.78554644  0.80654549  0.63865782]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.68027537  0.65148964  0.72025483  0.66838324]\n",
      " [ 0.70572441  0.63633402  0.66661668  0.65046284]\n",
      " [ 0.67432883  0.60000168  0.49349264  0.61149341]\n",
      " [ 0.66552492  0.60197246 -1.0091236   0.59818344]]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning using experience replay and target Q\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 40000\n",
    "size_experience_replay = 1000\n",
    "number_of_sample_from_experience_replay = 20\n",
    "time_period_to_update_target_Q = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q \n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen size_experience_replay for experience replay\n",
    "replay_meomory = deque(maxlen=size_experience_replay)\n",
    "\n",
    "# make a deque of maxlen size_experience_replay for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n",
    "\n",
    "# initialize target Q\n",
    "Q_target = Q\n",
    "\n",
    "# Q-learning using experience replay and target Q\n",
    "for t in range(epoch_q_learning):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:]) \n",
    "    \n",
    "    # time log to update target Q\n",
    "    time_log_to_update_target_Q = 0\n",
    "    \n",
    "    while not done:\n",
    "        # exploit - update Q-function using Q-learning with experience replay\n",
    "        # and\n",
    "        # explore - move according to updated epsilon-greedy policy\n",
    "        \n",
    "        # time log to update target Q\n",
    "        time_log_to_update_target_Q += 1\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # Q-learning using experience replay and target Q\n",
    "        # choose number_of_sample_from_experience_replay experiences from the deque\n",
    "        sample = random.sample(replay_meomory, number_of_sample_from_experience_replay)\n",
    "        for i in range(number_of_sample_from_experience_replay):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            \n",
    "            # Q-learning\n",
    "            # Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "            #                      alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "            # Q-learning with target Q\n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * max(Q_target[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "        # target Q update\n",
    "        if time_log_to_update_target_Q % time_period_to_update_target_Q == 0:\n",
    "            Q_target = Q\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Improvements since Nature DQN.png\" width=\"60%\" height=\"20%\"></div>\n",
    "\n",
    "http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Asynchronous Method\n",
    "\n",
    "최근에는 Asynchronous Method라는 방법으로 correlation을 없애줘서 Experience Replay를 대체한다고 한다. 간단히 Asynchronous Method를 설명하면 Thread를 통해서 여러개의 agent가 동시에 [state, action, reward, state’]를 수집한다. 그렇게 여러 agent가 동시에 수집한 데이터들은 서로 correlation이 없을 것이기 때문에 Experience Replay를 대체할 수 있으면서 더 빠르고 메모리도 절약할 수 있는 방법이라고 한다.\n",
    "\n",
    "http://www.phrgcm.com/blog/2016/08/17/deep-q-network/\n",
    "\n",
    "https://arxiv.org/pdf/1602.01783v2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Double DQN code.png\" width=\"100%\" height=\"50%\"></div>\n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.88294107e-02   5.80533475e-01   6.47216574e-02   2.03804496e-02]\n",
      " [  7.88383282e-02   7.38173943e-01   1.15983541e-01   1.49778303e-01]\n",
      " [  9.85908289e-02   8.06146385e-01   2.28119229e-01   5.76145462e-02]\n",
      " [  1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00]\n",
      " [  2.50531768e-02   1.16886068e-02   3.60279813e-01  -9.73112744e-03]\n",
      " [  1.18801188e-01  -2.32804519e-01   7.56112831e-01   2.60616980e-02]\n",
      " [ -1.00000000e+00  -1.00000000e+00  -1.00000000e+00  -1.00000000e+00]\n",
      " [ -1.34181063e-02   3.97168593e-04   8.47549702e-02  -1.29663057e-02]\n",
      " [ -6.25033897e-04   1.61342294e-01  -7.03326851e-03  -2.17996660e-03]\n",
      " [ -1.70211389e-03  -4.24904390e-02   3.65520181e-01   2.95034345e-02]\n",
      " [  1.55335945e-01   6.53493260e-04  -2.65430759e-01   5.09829324e-03]]\n",
      "[[ 0.64057426  0.66841556  0.64109863  0.61962404]\n",
      " [ 0.63909739  0.70765271  0.66796135  0.67067979]\n",
      " [ 0.66830872  0.76361993  0.71434224  0.29980248]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.61853367  0.61923261  0.64484412  0.61815823]\n",
      " [ 0.68601682 -0.77591591  0.71700293  0.58717138]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.5927597   0.58683463  0.61881191  0.59270986]\n",
      " [ 0.59474537  0.59051394  0.58709815  0.58826881]\n",
      " [ 0.57819051  0.56098296  0.53702469  0.56606841]\n",
      " [ 0.58899114  0.57921078 -0.74012386  0.58222636]]\n"
     ]
    }
   ],
   "source": [
    "# Double DQN\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 50000\n",
    "size_experience_replay = 1000\n",
    "number_of_sample_from_experience_replay = 20\n",
    "time_period_to_update_target_Q = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# policy\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# Q \n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  \n",
    "        \n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen size_experience_replay for experience replay\n",
    "replay_meomory = deque(maxlen=size_experience_replay)\n",
    "\n",
    "# make a deque of maxlen size_experience_replay for experience replay\n",
    "for t in range(epoch_sarsa):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "\n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # SARSA\n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n",
    "\n",
    "# initialize target Q\n",
    "Q_target = Q\n",
    "\n",
    "# Double DQN\n",
    "for t in range(epoch_q_learning):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:]) \n",
    "    \n",
    "    # time log to update target Q\n",
    "    time_log_to_update_target_Q = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # time log to update target Q\n",
    "        time_log_to_update_target_Q += 1\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        \n",
    "        # append current experience at the end of the deque and update the deque\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # Q-learning using experience replay and target Q\n",
    "        # choose number_of_sample_from_experience_replay experiences from the deque\n",
    "        sample = random.sample(replay_meomory, number_of_sample_from_experience_replay)\n",
    "        for i in range(number_of_sample_from_experience_replay):\n",
    "            # experience replay\n",
    "            replay = sample[i]\n",
    "            \n",
    "            # Q-learning\n",
    "            # Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "            #                      alpha * (replay[2] + gamma * max(Q[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "            # Q-learning with target Q\n",
    "            # Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "            #                      alpha * (replay[2] + gamma * max(Q_target[replay[3],:]) - Q[replay[0],replay[1]])\n",
    "                \n",
    "            # Double DQN\n",
    "            Double_DQN_action = np.argmax(Q[replay[3],:]) \n",
    "            Q[replay[0],replay[1]] = Q[replay[0],replay[1]] + \\\n",
    "                                 alpha * (replay[2] + gamma * Q_target[replay[3],Double_DQN_action] - Q[replay[0],replay[1]])\n",
    "                \n",
    "        # target Q update\n",
    "        if time_log_to_update_target_Q % time_period_to_update_target_Q == 0:\n",
    "            Q_target = Q\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dueling DQN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{ccccccccc}\n",
    "Q(s,a)&=&V(s)&+&A(s,a)\\\\\n",
    "\\uparrow&&\\uparrow&&\\uparrow\\\\\n",
    "\\mbox{$Q$ function}&&\\mbox{value function}&&\\mbox{advantage function}\\\\\n",
    "\\end{array}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/1-N_t9I7MeejAoWlDuH1i7cw.png\" width=\"30%\" height=\"10%\"></div>\n",
    "\n",
    "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
