{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SARSA and Q-learning\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model vs Model-free\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{llllll}\n",
    "\\mbox{Model}&\\quad\\Rightarrow\\quad&\\mbox{Model-free}\\\\\n",
    "\\mbox{Based on $P_{ss'}^a$}&\\quad\\Rightarrow\\quad&\\mbox{Based on Samples}\\\\\n",
    "V&\\quad\\Rightarrow\\quad&Q\\\\\n",
    "\\mbox{Greedy}&\\quad\\Rightarrow\\quad&\\mbox{$\\varepsilon$-Greedy}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Model\n",
    "\n",
    "If we know $R_s^a$, $P_{ss'}^a$, and $V$, and if we are at state $s$, our next action is\n",
    "$$\n",
    "\\mbox{argmax}_a Q(s,a) \\quad =\\quad \\mbox{argmax}_a\\left(R_s^a + \\gamma * \\sum_{s'} P_{ss'}^a * V(s')\\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Model-free\n",
    "\n",
    "- In reality, typically we don't know $P_{ss'}^a$.\n",
    "So, we cannot decide our next action based on $V$.\n",
    "That is why we use $Q$, not $V$.\n",
    "\n",
    "- If we update policy greedily, we may miss good regions in state space.\n",
    "We update policy $\\varepsilon$-greedily instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On and Off-Policy Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### On-policy learning\n",
    "\n",
    "- “Learn on the job”\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\pi$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Off-policy learning\n",
    "\n",
    "- “Look over someone’s shoulder”\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Sample $V$|Sample $Q$|Sample $Q$ (off-policy)|\n",
    "|---|---|\n",
    "|MC|MC|\n",
    "|TD|SARSA|Q-learnig|\n",
    "|TD($\\lambda$)|SARSA($\\lambda$)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SARSA \n",
    "\n",
    "With $a_{t+1}$ from the data\n",
    "$$\n",
    "Q(s_t,a_t)\\quad\\leftarrow\\quad\n",
    "Q(s_t,a_t)+\\alpha(\\color{red}{r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})}-Q(s_t,a_t))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64950299  0.69017939  0.64939369  0.63193186]\n",
      " [ 0.65124177  0.72248816  0.66564699  0.66779558]\n",
      " [ 0.66797505  0.76349194  0.68449087  0.58899867]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.63078184  0.6282633   0.66567924  0.60356169]\n",
      " [ 0.69930416 -0.64837452  0.74738361  0.55217622]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.60415412  0.58117413  0.64038186  0.60503118]\n",
      " [ 0.6186548   0.56715898  0.5799376   0.58076371]\n",
      " [ 0.58860248  0.40910139  0.53398223  0.55080234]\n",
      " [ 0.55843397  0.5218148  -0.86600922  0.53349094]]\n"
     ]
    }
   ],
   "source": [
    "# SARSA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "epoch = 30000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "# Q = 0.01*np.random.random((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # transition probability\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "#print(P)\n",
    "\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "\n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_policy_now_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_prob_minus_random_coin) if i>0 ][0]\n",
    "    \n",
    "for t in range(epoch):\n",
    "    \n",
    "    done = False\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    while not done:\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1,:])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1+4*epsilon)\n",
    "        \n",
    "        # choose action using epsilon-greedy policy \n",
    "        a1 = sample_action(policy_given_state=policy_now) \n",
    "        Q[s,a] = Q[s,a] + alpha * (R[s,a]+gamma*Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-learnig \n",
    "\n",
    "With a sampling $a'$ from the policy of interest, not from the data\n",
    "$$\n",
    "Q(s_t,a_t)\\quad\\leftarrow\\quad\n",
    "Q(s_t,a_t)+\\alpha(\\color{red}{r_{t+1}+\\gamma Q(s_{t+1},a')}-Q(s_t,a_t))\n",
    "$$\n",
    "\n",
    "If the policy of interest is greedy,\n",
    "$$\n",
    "Q(s_t,a_t)\\quad\\leftarrow\\quad\n",
    "Q(s_t,a_t)+\\alpha(\\color{red}{r_{t+1}+\\gamma \\max_{a'}Q(s_{t+1},a')}-Q(s_t,a_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05498236  0.54533103  0.07056754  0.03224705]\n",
      " [ 0.07911819  0.70386938  0.14433013  0.13376335]\n",
      " [ 0.1112421   0.79432344  0.2396675   0.08519836]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.02083935  0.01809598  0.3324215  -0.00364398]\n",
      " [ 0.15757644 -0.20682696  0.73833763  0.02644631]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [-0.00855757  0.01510166  0.08693695 -0.00556295]\n",
      " [-0.00731839  0.21470185 -0.00184952  0.00212867]\n",
      " [ 0.0079362  -0.01800231  0.42161678  0.05253249]\n",
      " [ 0.19009332  0.01269543 -0.23225642  0.00846619]]\n",
      "[[ 0.4585549   0.76560936  0.45227328  0.42416659]\n",
      " [ 0.44748585  0.78950609  0.49530373  0.51888518]\n",
      " [ 0.49591137  0.75993222  0.62025129  0.34950453]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.39306242  0.39920305  0.732524    0.38897126]\n",
      " [ 0.54592265 -0.70593815  0.81264371  0.33907882]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.35676667  0.3080031   0.67916175  0.34497249]\n",
      " [ 0.55154856  0.37293773  0.28650671  0.29991655]\n",
      " [ 0.37714219  0.24254151  0.22560698  0.28822026]\n",
      " [ 0.36824605  0.34027213 -0.93933829  0.33842648]]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "replay_meomory = deque(maxlen=100)\n",
    "epoch_sarsa = 1000\n",
    "epoch_q_learning = 20000\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "epsilon = 0.01\n",
    "\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "policy = 0.25 * np.ones((N_STATES, N_ACTIONS))\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "# Q = 0.01*np.random.random((N_STATES, N_ACTIONS))\n",
    "Q[3, :] = 1\n",
    "Q[6, :] = -1\n",
    "\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # transition probability\n",
    "\n",
    "P[0, 0, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 1, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0, 3, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[1, 0, :] = [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 1, :] = [0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0, 0]\n",
    "P[1, 2, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1, 3, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[2, 0, :] = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 1, :] = [0, 0, 0, 0.9, 0, 0, 0.1, 0, 0, 0, 0]\n",
    "P[2, 2, :] = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[2, 3, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "\n",
    "P[3, 0, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 1, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 2, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3, 3, :] = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "P[4, 0, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 1, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 2, :] = [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[4, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0]\n",
    "\n",
    "P[5, 0, :] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[5, 1, :] = [0, 0, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0.1]\n",
    "P[5, 2, :] = [0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[5, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1]\n",
    "\n",
    "P[6, 0, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 1, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 2, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[6, 3, :] = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "P[7, 0, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "P[7, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7, 2, :] = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[7, 3, :] = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "P[8, 0, :] = [0, 0, 0, 0, 0.1, 0, 0, 0.9, 0, 0, 0]\n",
    "P[8, 1, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[8, 2, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[8, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "P[9, 0, :] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[9, 1, :] = [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]\n",
    "P[9, 2, :] = [0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0]\n",
    "P[9, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "P[10, 0, :] = [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0]\n",
    "P[10, 1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[10, 2, :] = [0, 0, 0, 0, 0, 0.1, 0.9, 0, 0, 0, 0]\n",
    "P[10, 3, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "# print(P)\n",
    "\n",
    "if True:  # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "else:  # fuel-inefficient robot\n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "\n",
    "\n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_policy_now_minus_random_coin) if i > 0][0]\n",
    "\n",
    "\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_prob_minus_random_coin) if i > 0][0]\n",
    "\n",
    "\n",
    "for t in range(epoch_sarsa):\n",
    "\n",
    "    done = False\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  # 3 and 6 removed\n",
    "    a = sample_action(policy_given_state=policy[s, :])\n",
    "    while not done:\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using epsilon-greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        Q[s, a] = Q[s, a] + alpha * (R[s, a] + gamma * Q[s1, a1] - Q[s, a])\n",
    "\n",
    "        replay_meomory.append([s,a,R[s,a],s1])\n",
    "\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)\n",
    "\n",
    "for t in range(epoch_q_learning):\n",
    "\n",
    "    done = False\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    a = sample_action(policy_given_state=policy[s,:]) # epsilon-greedy policy\n",
    "    while not done:\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s,a,:])\n",
    "\n",
    "        # epsilon-greedy policy update\n",
    "        policy_now = np.zeros(N_ACTIONS)\n",
    "        m = np.argmax(Q[s1, :])\n",
    "        policy_now[m] = 1\n",
    "        policy_now = (policy_now + epsilon) / (1 + 4 * epsilon)\n",
    "\n",
    "        # choose action using greedy policy\n",
    "        a1 = sample_action(policy_given_state=policy_now)\n",
    "        replay_meomory.append([s, a, R[s, a], s1])\n",
    "\n",
    "        # experience replay\n",
    "        sample = random.sample(replay_meomory, 7)\n",
    "        for i in range(7):\n",
    "            sampled = sample[i]\n",
    "            Q[sampled[0],sampled[1]] = Q[sampled[0],sampled[1]] + \\\n",
    "                                 alpha * (sampled[2] + gamma * max(Q[sampled[3],:]) - Q[sampled[0],sampled[1]])\n",
    "\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
