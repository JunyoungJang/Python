{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q-network with experience replay\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0]\n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1]\n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))\n",
    "else: # fuel-inefficient robot\n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "\n",
    "# discount factor\n",
    "dis = 0.99\n",
    "\n",
    "# set parameters ###############################################################\n",
    "max_episodes = 4000\n",
    "learning_rate = 1e-1\n",
    "input_size = N_STATES\n",
    "output_size = N_ACTIONS\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# policy\n",
    "random_policy = 0.25*np.ones((1, N_ACTIONS))\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# define a function - sample_action\n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_policy_now - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_minus_coin = cum_prob - random_coin\n",
    "    return [n for n, i in enumerate(cum_minus_coin) if i > 0][0]\n",
    "\n",
    "# make a memory for a deque of maxlen 100 for experience replay\n",
    "replay_meomory = deque(maxlen=100)\n",
    "\n",
    "# define a function - one_hot\n",
    "def one_hot(state):\n",
    "    return np.identity(N_STATES)[state]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "Qpred = tf.matmul(X, W1)\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Setting up our environment\n",
    "init = tf.global_variables_initializer()\n",
    "# tf.get_default_graph().finalize()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "step_history = []\n",
    "for episode in range(max_episodes):\n",
    "    print(episode)\n",
    "    e = 1. / ((episode / 10) + 1)\n",
    "    step_count = 0\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    s_one_hot = one_hot(s)\n",
    "    done = False\n",
    "    # print(s)\n",
    "    # print(s_one_hot)\n",
    "\n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        x = np.reshape(s_one_hot, [1, input_size])\n",
    "        # Choose an action by greedily (with e chance of random action) from\n",
    "        # the Q-network\n",
    "        Q = sess.run(Qpred, feed_dict={X: x})\n",
    "        if np.random.rand(1) < e:\n",
    "            a = sample_action(policy_given_state=random_policy)\n",
    "        else:\n",
    "            a = np.argmax(Q)\n",
    "        # print(a)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        s1 = sample_transition(transition_prob_given_state_and_action=P[s, a, :])\n",
    "        s1_one_hot = one_hot(s1)\n",
    "        if (s1 == 3):\n",
    "            Q[0, a] = R[s, a] + dis * 1\n",
    "            done = True\n",
    "            step_history.append(1)\n",
    "            # append current experience at the end of the deque and update the deque\n",
    "            replay_meomory.append([s, a, Q, s1])\n",
    "        elif (s1 == 6):\n",
    "            Q[0, a] = R[s, a] + dis * (-1)\n",
    "            done = True\n",
    "            step_history.append(0)\n",
    "            # append current experience at the end of the deque and update the deque\n",
    "            replay_meomory.append([s, a, Q, s1])\n",
    "        else:\n",
    "            reward = R[s, a]\n",
    "            x_next = np.reshape(s1_one_hot, [1, input_size])\n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n",
    "            Q[0, a] = reward + dis * np.max(Q_next)\n",
    "            # append current experience at the end of the deque and update the deque\n",
    "            replay_meomory.append([s, a, Q, s1])\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "\n",
    "        # Q-learning using experience replay\n",
    "        sample = random.sample(replay_meomory, 1)\n",
    "        #print(sample)\n",
    "        x = np.reshape(one_hot(sample[0][0]), [1, input_size])\n",
    "        Q = sample[0][2]\n",
    "        # Train our network using target and predicted Q values on each episode\n",
    "        sess.run(train, feed_dict={X: x, Y: Q})\n",
    "\n",
    "history = np.cumsum(step_history) / (np.arange(max_episodes)+1)\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
