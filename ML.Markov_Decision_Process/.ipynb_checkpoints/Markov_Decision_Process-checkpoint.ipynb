{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Markov Decision Process\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Lecture 16 | Machine Learning (Stanford) [Andrew Ng](https://www.youtube.com/watch?v=RtxI449ZjSc)\n",
    "\n",
    "- Deep Reinforcement Learning [Pieter Abbeel](https://www.youtube.com/watch?v=ID150Tl-MMw)\n",
    "\n",
    "- A Tutorial on Reinforcement Learning by Emma Brunskill [1](https://www.youtube.com/watch?v=fIKkhoI1kF4) [2](https://www.youtube.com/watch?v=8hK0NnG_DhY)\n",
    "\n",
    "- Reinforcement Learning: An Introduction, Second edition, in progress [Richard S. Sutton and Andrew G. Barto](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf?dl=0) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning - An Introduction - Second edition.pdf) [code](https://github.com/dennybritz/reinforcement-learning)\n",
    "\n",
    "- Algorithms for Reinforcement Learning [Szepesvari](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs-lecture.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms for Reinforcement Learning.pdf)\n",
    "\n",
    "- Fundamental of Reinforcement Learning 이웅원 [ebook](https://www.gitbook.com/book/dnddnjs/rl/details) [code](https://github.com/rlcode/reinforcement-learning)\n",
    "\n",
    "- Awesome Reinforcement Learning [Hyunsoo Kim, Jiwon Kim](http://aikorea.org/awesome-rl/)\n",
    "\n",
    "- dennybritz/reinforcement-learning [code](https://github.com/dennybritz/reinforcement-learning)\n",
    "\n",
    "- Machine Learning, Tom Mitchell, 1997\n",
    "\n",
    "- [Simple Reinforcement Learning with TensorFlow](https://medium.com/emergent-future/)\n",
    "\n",
    "- [Simple reinforcement learning methods to learn CartPole](http://kvfrans.com/simple-algoritms-for-solving-cartpole/)\n",
    "\n",
    "- Deep Reinforcement Learning: Pong from Pixels [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/)\n",
    "\n",
    "- Reinforcement-learning [rlcode](https://github.com/dennybritz/reinforcement-learning)\n",
    "\n",
    "- [Python implementation of algorithms from Russell And Norvig's \"Artificial Intelligence - A Modern Approach\"](https://github.com/aimacode/aima-python)\n",
    "\n",
    "- How Does DeepMind's AlphaGo Zero Work? [Siraj Raval\n",
    "](https://www.youtube.com/watch?v=vC66XFoN4DE)\n",
    "\n",
    "- AI_CS188_MDP  [mebusy](https://github.com/mebusy/notes/blob/master/dev_notes/AI_CS188_MDP.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/move1.gif\" width=\"80%\" height=\"30%\"></div>\n",
    "\n",
    "https://qzprod.files.wordpress.com/2016/03/move1.gif?w=641"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reinforcement learning - Demo\n",
    "\n",
    "- [Stanford Autonomous Helicopter](http://heli.stanford.edu/) \n",
    "\n",
    "- [Real-world demonstrations of Reinforcement Learning](http://www.dcsc.tudelft.nl/~robotics/media.html)\n",
    "\n",
    "- [Deep Q-Learning Demo - A deep Q learning demonstration using ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html)\n",
    "\n",
    "- [Deep Q-Learning with Tensor Flow - A deep Q learning demonstration using Google Tensorflow](https://github.com/siemanko/tensorflow-deepq)\n",
    "\n",
    "- Google DeepMind's Deep Q-Learning & Superhuman Atari Gameplays [Two Minute Papers #27](https://www.youtube.com/watch?v=Ih8EfvOzBOY) [1](https://www.youtube.com/watch?v=GACcbfUaHwc) [2](https://www.youtube.com/watch?v=tkgPvAzUbzk) [3](https://www.youtube.com/watch?v=_LEthduIbtk) [4](https://www.youtube.com/watch?v=T58HkwX-OuI) [5](https://www.youtube.com/watch?v=iqXKQf2BOSE&feature=youtu.be)\n",
    "\n",
    "- [OpenAI Gym](https://gym.openai.com/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "- Data $\\{(x^{(i)},y^{(i)})\\}$\n",
    "\n",
    "- Goal: Find a function $f$ that explains well the relation of all inputs $x^{(i)}$ and outputs $y^{(i)}$.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "- Data $\\{x^{(i)}\\}$\n",
    "\n",
    "- Goal: Find a structure that explains well data.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "- Data $\\{x^{(i)}\\ \\mbox{or sometimes}\\ (x^{(i)},y^{(i)})\\}$\n",
    "\n",
    "- Goal: Find a good policy that wins the game most of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process (MDP)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/breakout.gif\" width=\"30%\" height=\"30%\"></div>\n",
    "\n",
    "http://ikuz.eu/wp-content/uploads/2015/02/breakout.gif\n",
    "\n",
    "DQN Breakout [DeepMind](https://www.youtube.com/watch?v=TmPfTpjtdgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/Markov Decision Process 2.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.09.22 PM.png\" width=\"60%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|0|1|2|3|\n",
    "|------|------|------|------|\n",
    "|4|H|5|6|\n",
    "|7|8|9|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0: Left\n",
    "\n",
    "- 1: Right\n",
    "\n",
    "- 2: Up\n",
    "\n",
    "- 3: Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transition probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You move according to your action with 80$\\%$ probability. \n",
    "\n",
    "- Your move may have a left and right one click error with 10$\\%$ probability each. \n",
    "\n",
    "- If there is a barrier against your move, your move bounds back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0.02 for each action.\n",
    "\n",
    "- If you reach the state 3, you win and get the final reward 1 at the end step.\n",
    "\n",
    "- If you reach the state 6, you lose and get the final reward -1 at the end step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discount factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\gamma=0.99$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0]\n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1]\n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) \n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/2007-164-water-policy.jpg\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "- When you are at state $s$, there are many actions you can choose.\n",
    "\n",
    "- Policy descibe how you choose your action.\n",
    "\n",
    "http://www.inkcinct.com.au/web-pages/cartoons/past/2007/2007-164-water-policy.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy in Andrew Ng's Lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2016-12-16+15.11.27.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "http://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/56381d00e4b05b1abc31cd96/58546ed09de4bb1925de9469/1494102176399/Screenshot+2016-12-16+15.11.27.png?format=1000w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bad policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Downarrow$|H|$\\Rightarrow$|-1|\n",
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Uparrow$|$\\Uparrow$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,0,1]\n",
    "policy[5,:] = [0,1,0,0]\n",
    "policy[6,:] = [0,1,0,0]\n",
    "policy[7,:] = [0,1,0,0]\n",
    "policy[8,:] = [0,1,0,0]\n",
    "policy[9,:] = [0,0,1,0]\n",
    "policy[10,:] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Uparrow$|H|$\\Uparrow$|-1|\n",
    "|$\\Uparrow$|$\\Leftarrow$|$\\Leftarrow$|$\\Leftarrow$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,1,0]\n",
    "policy[5,:] = [0,0,1,0]\n",
    "policy[6,:] = [0,0,1,0]\n",
    "policy[7,:] = [0,0,1,0]\n",
    "policy[8,:] = [1,0,0,0]\n",
    "policy[9,:] = [1,0,0,0]\n",
    "policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Policy in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# policy\n",
    "if False: # bad policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate random samples from discrete distribution - Inverse transform sampling or Smirov transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/3.png\" width=\"80%\"></div>\n",
    "\n",
    "https://www.slideshare.net/databricks/smart-scalable-feature-reduction-with-random-forests-with-erik-erlandson?from_action=save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_x\n",
    "def sample_x(probability_mass_function):\n",
    "    pmf = probability_mass_function\n",
    "    cdf = np.cumsum(pmf)\n",
    "    uni = np.random.random(1)\n",
    "    cdf_minus_uni = cdf - uni\n",
    "    return [ n for n,i in enumerate(cdf_minus_uni) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set parameters ###############################################################\n",
    "x   = [ -3,  -1,   1,   2,   5]\n",
    "pmf = [0.1, 0.1, 0.1, 0.5, 0.2]\n",
    "n_sim = 1000\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generate samples from a discrete random variable\n",
    "x_sample = []\n",
    "for _ in range(n_sim): \n",
    "    temp = sample_x(probability_mass_function=pmf)\n",
    "    x_temp = x[temp]\n",
    "    x_sample.append(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzqCXpM4Z9JLUOYNekjr3f1/m0mc2wkoJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f8749e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate histogram\n",
    "plt.hist(x_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npM4Z9JLUOYNekjr3v7TjysPo/WGkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ecba898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inverse transform sampling or Smirov transform\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define a function - sample_x\n",
    "def sample_x(probability_mass_function):\n",
    "    pmf = probability_mass_function\n",
    "    cdf = np.cumsum(pmf)\n",
    "    uni = np.random.random(1)\n",
    "    cdf_minus_uni = cdf - uni\n",
    "    return [ n for n,i in enumerate(cdf_minus_uni) if i>0 ][0]\n",
    "\n",
    "# set parameters ###############################################################\n",
    "x   = [ -3,  -1,   1,   2,   5]\n",
    "pmf = [0.1, 0.1, 0.1, 0.5, 0.2]\n",
    "n_sim = 1000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# generate samples from a discrete random variable\n",
    "x_sample = []\n",
    "for _ in range(n_sim): \n",
    "    temp = sample_x(probability_mass_function=pmf)\n",
    "    x_temp = x[temp]\n",
    "    x_sample.append(x_temp)\n",
    "\n",
    "# generate histogram\n",
    "plt.hist(x_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Simulation of MDP in Andrew Ng's Lecture 16.png\" width=\"80%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set parameters ###############################################################\n",
    "epoch = 2\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# policy\n",
    "if False: # bad policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_policy_now_minus_random_coin) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_prob_minus_random_coin) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MDP simulation\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(\n",
    "            transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # choose action using current policy\n",
    "        a1 = sample_action(policy_given_state=policy[s1,:]) \n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3):\n",
    "            final_reward = R[s1,a1] + gamma * 1\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        elif (s1 == 6):\n",
    "            final_reward = R[s1,a1] + gamma * (-1)\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        else:\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,R[s1,a1],False))\n",
    "            s = s1\n",
    "            a = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  7, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  4, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  0, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  1, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  2, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  3, Action:  1, Reward:   0.97, Done: True\n",
      "State:  8, Action:  0, Reward:  -0.02, Done: False\n",
      "State:  7, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  4, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  0, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  1, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  2, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  3, Action:  1, Reward:   0.97, Done: True\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 2\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "       \n",
    "# policy\n",
    "if False: # bad policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_policy_now_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_prob_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# MDP simulation\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(\n",
    "            transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # choose action using current policy\n",
    "        a1 = sample_action(policy_given_state=policy[s1,:]) \n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3):\n",
    "            final_reward = R[s1,a1] + gamma * 1\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        elif (s1 == 6):\n",
    "            final_reward = R[s1,a1] + gamma * (-1)\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        else:\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,R[s1,a1],False))\n",
    "            s = s1\n",
    "            a = a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy.png\" width=\"80%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# code skipped ###\n",
    "\n",
    "# MDP simulation\n",
    "simulation_history = []\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # code skipped ###\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # code skipped ###\n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3):\n",
    "            done = True\n",
    "            simulation_history.append(1)  \n",
    "        elif (s1 == 6):\n",
    "            done = True\n",
    "            simulation_history.append(0)\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "                     \n",
    "history = np.cumsum(simulation_history) / (np.arange(epoch)+1)\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10efda898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8977\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 10000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "       \n",
    "# policy\n",
    "if False: # bad policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif False: # optimal policy + noise \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,0.9,0.1,0]\n",
    "    policy[1,:] = [0,0.9,0.1,0]\n",
    "    policy[2,:] = [0,0.9,0.1,0]\n",
    "    policy[3,:] = [0,0.9,0.1,0]\n",
    "    policy[4,:] = [0,0,0.9,0.1]\n",
    "    policy[5,:] = [0,0,0.9,0.1]\n",
    "    policy[6,:] = [0,0,0.9,0.1]\n",
    "    policy[7,:] = [0,0,0.9,0.1]\n",
    "    policy[8,:] = [0.9,0.1,0,0]\n",
    "    policy[9,:] = [0.9,0.1,0,0]\n",
    "    policy[10,:] = [0.9,0.1,0,0]\n",
    "elif True: # optimal policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_policy_now_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_prob_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# MDP simulation\n",
    "simulation_history = []\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(\n",
    "            transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # choose action using current policy\n",
    "        a1 = sample_action(policy_given_state=policy[s1,:]) \n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3):\n",
    "            done = True\n",
    "            simulation_history.append(1)  \n",
    "        elif (s1 == 6):\n",
    "            done = True\n",
    "            simulation_history.append(0)\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "                     \n",
    "history = np.cumsum(simulation_history) / (np.arange(epoch)+1)\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "print(history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Run the above MDP simulation code and draw the success rate curve for bad and random policy, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
