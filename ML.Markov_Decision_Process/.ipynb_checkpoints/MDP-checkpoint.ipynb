{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Markov Decision Process\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Lecture 16 | Machine Learning (Stanford) [Andrew Ng](https://www.youtube.com/watch?v=RtxI449ZjSc)\n",
    "\n",
    "- Deep Reinforcement Learning [Pieter Abbeel](https://www.youtube.com/watch?v=ID150Tl-MMw)\n",
    "\n",
    "- A Tutorial on Reinforcement Learning by Emma Brunskill [1](https://www.youtube.com/watch?v=fIKkhoI1kF4) [2](https://www.youtube.com/watch?v=8hK0NnG_DhY)\n",
    "\n",
    "- Reinforcement Learning: An Introduction, Second edition, in progress [Richard S. Sutton and Andrew G. Barto](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf?dl=0) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement Learning - An Introduction - Second edition.pdf) [code](https://github.com/dennybritz/reinforcement-learning)\n",
    "\n",
    "- Algorithms for Reinforcement Learning [Szepesvari](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs-lecture.pdf) [local-slide](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms for Reinforcement Learning.pdf)\n",
    "\n",
    "- Fundamental of Reinforcement Learning 이웅원 [ebook](https://www.gitbook.com/book/dnddnjs/rl/details) [code](https://github.com/rlcode/reinforcement-learning)\n",
    "\n",
    "- Awesome Reinforcement Learning [Hyunsoo Kim, Jiwon Kim](http://aikorea.org/awesome-rl/)\n",
    "\n",
    "- dennybritz/reinforcement-learning [code](https://github.com/dennybritz/reinforcement-learning)\n",
    "\n",
    "- Machine Learning, Tom Mitchell, 1997\n",
    "\n",
    "- [Simple Reinforcement Learning with TensorFlow](https://medium.com/emergent-future/)\n",
    "\n",
    "- [Simple reinforcement learning methods to learn CartPole](http://kvfrans.com/simple-algoritms-for-solving-cartpole/)\n",
    "\n",
    "- Deep Reinforcement Learning: Pong from Pixels [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/)\n",
    "\n",
    "- Reinforcement-learning [rlcode](https://github.com/dennybritz/reinforcement-learning)\n",
    "\n",
    "- [Python implementation of algorithms from Russell And Norvig's \"Artificial Intelligence - A Modern Approach\"](https://github.com/aimacode/aima-python)\n",
    "\n",
    "- How Does DeepMind's AlphaGo Zero Work? [Siraj Raval\n",
    "](https://www.youtube.com/watch?v=vC66XFoN4DE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/move1.gif\" width=\"80%\" height=\"30%\"></div>\n",
    "\n",
    "https://qzprod.files.wordpress.com/2016/03/move1.gif?w=641"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reinforcement learning - Demo\n",
    "\n",
    "- [Stanford Autonomous Helicopter](http://heli.stanford.edu/) \n",
    "\n",
    "- [Real-world demonstrations of Reinforcement Learning](http://www.dcsc.tudelft.nl/~robotics/media.html)\n",
    "\n",
    "- [Deep Q-Learning Demo - A deep Q learning demonstration using ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html)\n",
    "\n",
    "- [Deep Q-Learning with Tensor Flow - A deep Q learning demonstration using Google Tensorflow](https://github.com/siemanko/tensorflow-deepq)\n",
    "\n",
    "- Google DeepMind's Deep Q-Learning & Superhuman Atari Gameplays [Two Minute Papers #27](https://www.youtube.com/watch?v=Ih8EfvOzBOY) [1](https://www.youtube.com/watch?v=GACcbfUaHwc) [2](https://www.youtube.com/watch?v=tkgPvAzUbzk) [3](https://www.youtube.com/watch?v=_LEthduIbtk) [4](https://www.youtube.com/watch?v=T58HkwX-OuI) [5](https://www.youtube.com/watch?v=iqXKQf2BOSE&feature=youtu.be)\n",
    "\n",
    "- [OpenAI Gym](https://gym.openai.com/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process (MDP)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/breakout.gif\" width=\"50%\" height=\"30%\"></div>\n",
    "\n",
    "http://ikuz.eu/wp-content/uploads/2015/02/breakout.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/Markov Decision Process 2.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|0|1|2|3|\n",
    "|------|------|------|------|\n",
    "|4|H|5|6|\n",
    "|7|8|9|10|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|0|1|2|3|\n",
    "|------|------|------|------|\n",
    "|4|H|5|6|\n",
    "|7|8|9|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0: Left\n",
    "\n",
    "- 1: Right\n",
    "\n",
    "- 2: Up\n",
    "\n",
    "- 3: Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transition probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You move according to your action with 80$\\%$ probability. \n",
    "\n",
    "- Your move may have a left and right one click error with 10$\\%$ probability each. \n",
    "\n",
    "- If there is a barrier against your move, your move bounds back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# transition probability\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0.02 for each action.\n",
    "\n",
    "- If you reach the state 3, you win and get the final reward 1 at the end step.\n",
    "\n",
    "- If you reach the state 6, you lose and get the final reward -1 at the end step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# reward\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discount factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\gamma=0.99$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When you are at state $s$, there are many actions you can choose.\n",
    "\n",
    "- Policy descibe how you choose your action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy in Andrew Ng's Lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bad policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Downarrow$|H|$\\Rightarrow$|-1|\n",
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Uparrow$|$\\Uparrow$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,0,1]\n",
    "policy[5,:] = [0,1,0,0]\n",
    "policy[6,:] = [0,1,0,0]\n",
    "policy[7,:] = [0,1,0,0]\n",
    "policy[8,:] = [0,1,0,0]\n",
    "policy[9,:] = [0,0,1,0]\n",
    "policy[10,:] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Uparrow$|H|$\\Uparrow$|-1|\n",
    "|$\\Uparrow$|$\\Leftarrow$|$\\Leftarrow$|$\\Leftarrow$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,1,0]\n",
    "policy[5,:] = [0,0,1,0]\n",
    "policy[6,:] = [0,0,1,0]\n",
    "policy[7,:] = [0,0,1,0]\n",
    "policy[8,:] = [1,0,0,0]\n",
    "policy[9,:] = [1,0,0,0]\n",
    "policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate random samples from discrete distribution - Inverse transform sampling or Smirov transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/3.png\" width=\"80%\"></div>\n",
    "\n",
    "https://www.slideshare.net/databricks/smart-scalable-feature-reduction-with-random-forests-with-erik-erlandson?from_action=save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_x\n",
    "def sample_x(probability_mass_function):\n",
    "    pmf = probability_mass_function\n",
    "    cdf = np.cumsum(pmf)\n",
    "    uni = np.random.random(1)\n",
    "    cdf_minus_uni = cdf - uni\n",
    "    return [ n for n,i in enumerate(cdf_minus_uni) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set parameters ###############################################################\n",
    "x   = [ -3,  -1,   1,   2,   5]\n",
    "pmf = [0.1, 0.1, 0.1, 0.5, 0.2]\n",
    "n_sim = 1000\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generate samples from a discrete random variable\n",
    "x_sample = []\n",
    "for _ in range(n_sim): \n",
    "    temp = sample_x(probability_mass_function=pmf)\n",
    "    x_temp = x[temp]\n",
    "    x_sample.append(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzqCXpM4Z9JLUOYNekjr3f1/m0mc2wkoJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f8749e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate histogram\n",
    "plt.hist(x_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADq5JREFUeJzt3H+o3Xd9x/Hna039gTqjNgtdEnYFw0YRrXIpkcqY7ZT+\nENMNLYrT6AL5p0JFwcUJE9kGLYJV2XAEK8atU8u0NLSdNmsrIqzVG621NTrvSkoSqrlqW5WiI/re\nH+eTeYxJ7zm59/R7+9nzAZf7+Xy+n3M+73uT+7rf+znf801VIUnq1+8MXYAkabYMeknqnEEvSZ0z\n6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1Ln1g1dAMA555xTc3NzQ5chSU8pBw4c+GFVbVhu3kRB\nn+QQ8FPgl8DxqppP8nzgs8AccAi4sqoeSRLgI8BlwOPA26rq60/0/HNzcywsLExSiiSpSfLQJPOm\n2bp5VVWdX1Xzrb8buKOqtgJ3tD7ApcDW9rEL+NgUa0iSVtlK9ui3A3tbey9wxdj4p2rkbmB9knNX\nsI4kaQUmDfoCbk9yIMmuNraxqh5u7e8DG1t7E3B47LFH2thvSLIryUKShaWlpTMoXZI0iUlfjH1l\nVR1N8nvA/iTfGT9YVZVkqvsdV9UeYA/A/Py890qWpBmZ6Iy+qo62z8eAm4ALgB+c2JJpn4+16UeB\nLWMP39zGJEkDWDbokzwryXNOtIHXAPcD+4AdbdoO4ObW3ge8NSPbgMfGtngkSU+ySbZuNgI3ja6a\nZB3wr1X1hSRfA25MshN4CLiyzb+N0aWVi4wur3z7qlctSZrYskFfVQ8CLz3F+I+Ai08xXsBVq1Kd\nJGnFvAWCJHVuTdwCQdJvm9t96yDrHrrm8kHW1ex4Ri9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6\nZ9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMG\nvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BL\nUucmDvokZyX5RpJbWv+FSe5Jspjks0me1saf3vqL7fjcbEqXJE1imjP6q4GDY/1rgeuq6kXAI8DO\nNr4TeKSNX9fmSZIGMlHQJ9kMXA58vPUDXAT8W5uyF7iitbe3Pu34xW2+JGkAk57Rfxh4D/Cr1n8B\n8GhVHW/9I8Cm1t4EHAZoxx9r8yVJA1g26JO8FjhWVQdWc+Eku5IsJFlYWlpazaeWJI2Z5Iz+QuB1\nSQ4Bn2G0ZfMRYH2SdW3OZuBoax8FtgC0488FfnTyk1bVnqqar6r5DRs2rOiLkCSd3rJBX1XvrarN\nVTUHvBG4s6reDNwFvL5N2wHc3Nr7Wp92/M6qqlWtWpI0sZVcR/9XwLuSLDLag7++jV8PvKCNvwvY\nvbISJUkrsW75Kb9WVV8CvtTaDwIXnGLOz4E3rEJtkqRV4DtjJalzBr0kdc6gl6TOGfSS1DmDXpI6\nZ9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMG\nvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BL\nUucMeknqnEEvSZ0z6CWpcwa9JHVu2aBP8owkX03yzSQPJPlAG39hknuSLCb5bJKntfGnt/5iOz43\n2y9BkvREJjmj/wVwUVW9FDgfuCTJNuBa4LqqehHwCLCzzd8JPNLGr2vzJEkDWTboa+RnrXt2+yjg\nIuDf2vhe4IrW3t76tOMXJ8mqVSxJmspEe/RJzkpyL3AM2A/8N/BoVR1vU44Am1p7E3AYoB1/DHjB\nahYtSZrcREFfVb+sqvOBzcAFwB+tdOEku5IsJFlYWlpa6dNJkk5jqqtuqupR4C7gFcD6JOvaoc3A\n0dY+CmwBaMefC/zoFM+1p6rmq2p+w4YNZ1i+JGk5k1x1syHJ+tZ+JvBq4CCjwH99m7YDuLm197U+\n7fidVVWrWbQkaXLrlp/CucDeJGcx+sVwY1XdkuTbwGeS/B3wDeD6Nv964J+TLAI/Bt44g7olSRNa\nNuir6j7gZacYf5DRfv3J4z8H3rAq1UmSVsx3xkpS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS\n1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0md\nM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1Ll1\nQxcgSUOb233rYGsfuubyma/hGb0kdc6gl6TOLRv0SbYkuSvJt5M8kOTqNv78JPuTfK99fl4bT5KP\nJllMcl+Sl8/6i5Aknd4kZ/THgXdX1XnANuCqJOcBu4E7qmorcEfrA1wKbG0fu4CPrXrVkqSJLRv0\nVfVwVX29tX8KHAQ2AduBvW3aXuCK1t4OfKpG7gbWJzl31SuXJE1kqj36JHPAy4B7gI1V9XA79H1g\nY2tvAg6PPexIGzv5uXYlWUiysLS0NGXZkqRJTRz0SZ4NfA54Z1X9ZPxYVRVQ0yxcVXuqar6q5jds\n2DDNQyVJU5go6JOczSjkb6iqz7fhH5zYkmmfj7Xxo8CWsYdvbmOSpAFMctVNgOuBg1X1obFD+4Ad\nrb0DuHls/K3t6pttwGNjWzySpCfZJO+MvRB4C/CtJPe2sb8GrgFuTLITeAi4sh27DbgMWAQeB96+\nqhVLkqaybNBX1VeAnObwxaeYX8BVK6xLkrRKfGesJHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxB\nL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS\n1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0md\nM+glqXPrhi5gpeZ23zrY2oeuuXywtSVpUsue0Sf5RJJjSe4fG3t+kv1Jvtc+P6+NJ8lHkywmuS/J\ny2dZvCRpeZNs3XwSuOSksd3AHVW1Fbij9QEuBba2j13Ax1anTEnSmVo26Kvqy8CPTxreDuxt7b3A\nFWPjn6qRu4H1Sc5drWIlSdM70xdjN1bVw639fWBja28CDo/NO9LGJEkDWfFVN1VVQE37uCS7kiwk\nWVhaWlppGZKk0zjTq25+kOTcqnq4bc0ca+NHgS1j8za3sd9SVXuAPQDz8/NT/6L4/8wrjSRN40zP\n6PcBO1p7B3Dz2Phb29U324DHxrZ4JEkDWPaMPsmngT8BzklyBHg/cA1wY5KdwEPAlW36bcBlwCLw\nOPD2GdQsSZrCskFfVW86zaGLTzG3gKtWWpS0Vgy5TSatFm+BIEmdM+glqXMGvSR1zqCXpM4Z9JLU\nOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z\n6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNe\nkjpn0EtS5wx6SeqcQS9JnTPoJalzMwn6JJck+W6SxSS7Z7GGJGkyqx70Sc4C/hG4FDgPeFOS81Z7\nHUnSZGZxRn8BsFhVD1bV/wCfAbbPYB1J0gRmEfSbgMNj/SNtTJI0gHVDLZxkF7CrdX+W5Ltn+FTn\nAD9cnaqmk2uf8PBgdS1jRXUt8zWvRJffrxmbSW2r8G+8Vr9na7KuXLuiuv5gkkmzCPqjwJax/uY2\n9huqag+wZ6WLJVmoqvmVPs9qs67pWNf01mpt1jWdJ6OuWWzdfA3YmuSFSZ4GvBHYN4N1JEkTWPUz\n+qo6nuQdwBeBs4BPVNUDq72OJGkyM9mjr6rbgNtm8dynsOLtnxmxrulY1/TWam3WNZ2Z15WqmvUa\nkqQBeQsESepcF0Gf5G+T3Jfk3iS3J/n9oWsCSPLBJN9ptd2UZP3QNQEkeUOSB5L8KsngVyGsxVtm\nJPlEkmNJ7h+6lnFJtiS5K8m327/h1UPXBJDkGUm+muSbra4PDF3TuCRnJflGkluGruWEJIeSfKvl\n1sIs1+oi6IEPVtVLqup84Bbgb4YuqNkPvLiqXgL8F/Deges54X7gz4EvD13IGr5lxieBS4Yu4hSO\nA++uqvOAbcBVa+T79Qvgoqp6KXA+cEmSbQPXNO5q4ODQRZzCq6rq/Kfi5ZVPuqr6yVj3WcCaeOGh\nqm6vquOtezej9xQMrqoOVtWZvkFtta3JW2ZU1ZeBHw9dx8mq6uGq+npr/5RReA3+zvMa+Vnrnt0+\n1sTPYZLNwOXAx4euZShdBD1Akr9Pchh4M2vnjH7cXwL/PnQRa5C3zDhDSeaAlwH3DFvJSNseuRc4\nBuyvqjVRF/Bh4D3Ar4Yu5CQF3J7kQLtTwMw8ZYI+yX8kuf8UH9sBqup9VbUFuAF4x1qpq815H6M/\nuW9YS3XpqSvJs4HPAe886S/awVTVL9v26WbggiQvHrqmJK8FjlXVgaFrOYVXVtXLGW1bXpXkj2e1\n0GD3uplWVf3phFNvYHQN//tnWM7/Wa6uJG8DXgtcXE/itaxTfL+GNtEtM/RrSc5mFPI3VNXnh67n\nZFX1aJK7GL3GMfSL2RcCr0tyGfAM4HeT/EtV/cXAdVFVR9vnY0luYrSNOZPXzZ4yZ/RPJMnWse52\n4DtD1TIuySWM/mR8XVU9PnQ9a5S3zJhCkgDXAwer6kND13NCkg0nripL8kzg1ayBn8Oqem9Vba6q\nOUb/t+5cCyGf5FlJnnOiDbyGGf5S7CLogWvatsR9jL5ha+KSM+AfgOcA+9slVP80dEEASf4syRHg\nFcCtSb44VC3txeoTt8w4CNy4Fm6ZkeTTwH8Cf5jkSJKdQ9fUXAi8Bbio/Z+6t52tDu1c4K72M/g1\nRnv0a+ZSxjVoI/CVJN8EvgrcWlVfmNVivjNWkjrXyxm9JOk0DHpJ6pxBL0mdM+glqXMGvSR1zqCX\npM4Z9JLUOYNekjr3v7TjysPo/WGkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ecba898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inverse transform sampling or Smirov transform\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define a function - sample_x\n",
    "def sample_x(probability_mass_function):\n",
    "    pmf = probability_mass_function\n",
    "    cdf = np.cumsum(pmf)\n",
    "    uni = np.random.random(1)\n",
    "    cdf_minus_uni = cdf - uni\n",
    "    return [ n for n,i in enumerate(cdf_minus_uni) if i>0 ][0]\n",
    "\n",
    "# set parameters ###############################################################\n",
    "x   = [ -3,  -1,   1,   2,   5]\n",
    "pmf = [0.1, 0.1, 0.1, 0.5, 0.2]\n",
    "n_sim = 1000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# generate samples from a discrete random variable\n",
    "x_sample = []\n",
    "for _ in range(n_sim): \n",
    "    temp = sample_x(probability_mass_function=pmf)\n",
    "    x_temp = x[temp]\n",
    "    x_sample.append(x_temp)\n",
    "\n",
    "# generate histogram\n",
    "plt.hist(x_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Simulation of MDP in Andrew Ng's Lecture 16.png\" width=\"80%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set parameters ###############################################################\n",
    "epoch = 2\n",
    "# set parameters ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# policy\n",
    "if False: # bad policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_policy_now_minus_random_coin) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_prob_minus_random_coin) if i>0 ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MDP simulation\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(\n",
    "            transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # choose action using current policy\n",
    "        a1 = sample_action(policy_given_state=policy[s1,:]) \n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3):\n",
    "            final_reward = R[s1,a1] + gamma * 1\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        elif (s1 == 6):\n",
    "            final_reward = R[s1,a1] + gamma * (-1)\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        else:\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,R[s1,a1],False))\n",
    "            s = s1\n",
    "            a = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  7, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  4, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  0, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  1, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  2, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  3, Action:  1, Reward:   0.97, Done: True\n",
      "State:  8, Action:  0, Reward:  -0.02, Done: False\n",
      "State:  7, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  4, Action:  2, Reward:  -0.02, Done: False\n",
      "State:  0, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  1, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  2, Action:  1, Reward:  -0.02, Done: False\n",
      "State:  3, Action:  1, Reward:   0.97, Done: True\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 2\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  \n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "\n",
    "# rewards\n",
    "if True: # fuel-efficient robot\n",
    "    R = -0.02 * np.ones((N_STATES, N_ACTIONS))  \n",
    "else: # fuel-inefficient robot \n",
    "    R = -0.5 * np.ones((N_STATES, N_ACTIONS))\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "       \n",
    "# policy\n",
    "if False: # bad policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "# define a function - sample_action \n",
    "def sample_action(policy_given_state):\n",
    "    policy_now = policy_given_state\n",
    "    cum_policy_now = np.cumsum(policy_now)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_policy_now_minus_random_coin = cum_policy_now - random_coin \n",
    "    return [ n for n,i in enumerate(cum_policy_now_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# define a function - sample_transition\n",
    "def sample_transition(transition_prob_given_state_and_action):\n",
    "    prob = transition_prob_given_state_and_action\n",
    "    cum_prob = np.cumsum(prob)\n",
    "    random_coin = np.random.random(1)\n",
    "    cum_prob_minus_random_coin = cum_prob - random_coin \n",
    "    return [ n for n,i in enumerate(cum_prob_minus_random_coin) if i>0 ][0]\n",
    "\n",
    "# MDP simulation\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    # choose initial state randomly\n",
    "    s = np.random.choice([0,1,2,4,5,7,8,9,10]) # 3 and 6 removed\n",
    "    # choose action using current policy\n",
    "    a = sample_action(policy_given_state=policy[s,:])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = sample_transition(\n",
    "            transition_prob_given_state_and_action=P[s,a,:])\n",
    "        \n",
    "        # choose action using current policy\n",
    "        a1 = sample_action(policy_given_state=policy[s1,:]) \n",
    "\n",
    "        # if game is not over, continue playing game\n",
    "        if (s1 == 3):\n",
    "            final_reward = R[s1,a1] + gamma * 1\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        elif (s1 == 6):\n",
    "            final_reward = R[s1,a1] + gamma * (-1)\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,final_reward,True))\n",
    "            done = True\n",
    "        else:\n",
    "            print('State: {0:2}, Action: {1:2}, Reward: {2:6}, Done: {3}'.format(s1,a1,R[s1,a1],False))\n",
    "            s = s1\n",
    "            a = a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Run the above MDP simulation code many times and compute the winning rate for bad, random, and optimal policy, respectively. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
