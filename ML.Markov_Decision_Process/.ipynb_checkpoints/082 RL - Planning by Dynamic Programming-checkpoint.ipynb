{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python implementation of algorithms from Russell And Norvig's \"Artificial Intelligence - A Modern Approach\"](https://github.com/aimacode/aima-python)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Iterative Policy Evaluation.png\"/>\n",
    "\n",
    "# Bellman's expectation equation for $v_\\pi$ and $q_\\pi$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)q_\\pi(s,a)\\nonumber\\\\\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Policy Iteration.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Value Iteration.png\"/>\n",
    "\n",
    "# Bellman optimality equation for $v_{*}$ and $q_{*}$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}q_*(s,a)\\nonumber\\\\\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\max_{a'}q_*(s',a')\\right)\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronous Dynamic Programming\n",
    "\n",
    "<img src=\"img/Synchronous Dynamic Programming 2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Asynchronous Dynamic Programming\n",
    "\n",
    "### In-Place Dynamic Programming\n",
    "\n",
    "<img src=\"img/In-Place Dynamic Programming.png\"/>\n",
    "\n",
    "### Prioritised Sweeping\n",
    "\n",
    "<img src=\"img/Prioritised Sweeping.png\"/>\n",
    "\n",
    "### Real-Time Dynamic Programming\n",
    "\n",
    "<img src=\"img/Real-Time Dynamic Programming.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Andrew Ng - table 2.png\"/>\n",
    "\n",
    "\n",
    "|0|1|2|3|\n",
    "|------|------|------|------|\n",
    "|4|H|5|6|\n",
    "|7|8|9|10|\n",
    "\n",
    "<img src=\"img/Andrew Ng - table 3.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71576205  0.74319399  0.772       1.          0.69132019  0.76103021\n",
      " -1.          0.66440699  0.64042733  0.61402305  0.60243653]\n"
     ]
    }
   ],
   "source": [
    "# policy evaluation - v_\\pi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1\n",
    "V[6] = -1\n",
    "#print(policy)\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # transition probability\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "#print(P)\n",
    "\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "#print(R)\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        V[s] = sum([policy[s,a]*(R[s,a]+ gamma*sum([P[s,a,s1]*V[s1] for s1 in range(N_STATES)])) for a in range(N_ACTIONS)])\n",
    "    V[3] = 1\n",
    "    V[6] = -1\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65632878  0.76103021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41678095  0.55808791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.84456801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# policy evaluation - q_\\pi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "if False: # bad policy presented above (top)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy presented above (bottom)\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # transition probability\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "#print(P)\n",
    "\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "#print(R)\n",
    "\n",
    "for i in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*sum([policy[s1,a1]*Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65632878  0.76103021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41678095  0.55808791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.84456801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# value iteration - q_\\pi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "epoch = 100\n",
    "\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # transition probability\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "#print(P)\n",
    "\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "#print(R)\n",
    "\n",
    "for t in range(epoch):\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*max([Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "    Q[3,:] = 1\n",
    "    Q[6,:] = -1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68860443  0.71576205  0.68860443  0.66440699]\n",
      " [ 0.68618469  0.74319399  0.71576205  0.71576205]\n",
      " [ 0.71576205  0.772       0.74428     0.55907791]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.66440699  0.66440699  0.69132019  0.63538893]\n",
      " [ 0.7334199  -0.65632878  0.76103021  0.58934978]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63776292  0.61402305  0.66440699  0.63776292]\n",
      " [ 0.64042733  0.60243653  0.61402305  0.61402305]\n",
      " [ 0.61402305  0.41678095  0.55808791  0.58788282]\n",
      " [ 0.60243653  0.57641217 -0.84456801  0.57641217]]\n"
     ]
    }
   ],
   "source": [
    "# policy iteration - q_\\pi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "epoch = 100\n",
    "\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1\n",
    "Q[6,:] = -1\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # transition probability\n",
    "\n",
    "P[0,0,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,1,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[0,2,:] = [1,0,0,0,0,0,0,0,0,0,0]\n",
    "P[0,3,:] = [0,0,0,0,1,0,0,0,0,0,0]  \n",
    "\n",
    "P[1,0,:] = [0.9,0,0,0,0.1,0,0,0,0,0,0]\n",
    "P[1,1,:] = [0,0,0.9,0,0,0.1,0,0,0,0,0]\n",
    "P[1,2,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[1,3,:] = [0,1,0,0,0,0,0,0,0,0,0] \n",
    "\n",
    "P[2,0,:] = [0,1,0,0,0,0,0,0,0,0,0]\n",
    "P[2,1,:] = [0,0,0,0.9,0,0,0.1,0,0,0,0]\n",
    "P[2,2,:] = [0,0,1,0,0,0,0,0,0,0,0]\n",
    "P[2,3,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0] \n",
    "\n",
    "P[3,0,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,1,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,2,:] = [0,0,0,1,0,0,0,0,0,0,0]\n",
    "P[3,3,:] = [0,0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "P[4,0,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,1,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[4,2,:] = [0.9,0.1,0,0,0,0,0,0,0,0,0]\n",
    "P[4,3,:] = [0,0,0,0,0,0,0,0.9,0.1,0,0] \n",
    "\n",
    "P[5,0,:] = [0,0,0,0,0,1,0,0,0,0,0]\n",
    "P[5,1,:] = [0,0,0,0.1,0,0,0.8,0,0,0,0.1]\n",
    "P[5,2,:] = [0,0.1,0.8,0.1,0,0,0,0,0,0,0]\n",
    "P[5,3,:] = [0,0,0,0,0,0,0,0,0.1,0.8,0.1] \n",
    "\n",
    "P[6,0,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,1,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,2,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "P[6,3,:] = [0,0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "P[7,0,:] = [0,0,0,0,0,0,0,1,0,0,0]\n",
    "P[7,1,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[7,2,:] = [0,0,0,0,1,0,0,0,0,0,0]\n",
    "P[7,3,:] = [0,0,0,0,0,0,0,1,0,0,0] \n",
    "\n",
    "P[8,0,:] = [0,0,0,0,0.1,0,0,0.9,0,0,0]\n",
    "P[8,1,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[8,2,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[8,3,:] = [0,0,0,0,0,0,0,0,1,0,0] \n",
    "\n",
    "P[9,0,:] = [0,0,0,0,0,0,0,0,1,0,0]\n",
    "P[9,1,:] = [0,0,0,0,0,0,0.1,0,0,0,0.9]\n",
    "P[9,2,:] = [0,0,0,0,0,0.9,0.1,0,0,0,0]\n",
    "P[9,3,:] = [0,0,0,0,0,0,0,0,0,1,0] \n",
    "\n",
    "P[10,0,:] = [0,0,0,0,0,0.1,0,0,0,0.9,0]\n",
    "P[10,1,:] = [0,0,0,0,0,0,0,0,0,0,1]\n",
    "P[10,2,:] = [0,0,0,0,0,0.1,0.9,0,0,0,0]\n",
    "P[10,3,:] = [0,0,0,0,0,0,0,0,0,0,1] \n",
    "#print(P)\n",
    "\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS))  # rewards\n",
    "#print(R)\n",
    "\n",
    "for t in range(epoch):\n",
    "\n",
    "    # evaluate policy\n",
    "    for i in range(100):\n",
    "        for s in range(N_STATES):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Q[s,a] = R[s,a]+gamma*sum([P[s,a,s1]*sum([policy[s1,a1]*Q[s1,a1] for a1 in range(N_ACTIONS)]) for s1 in range(N_STATES)])\n",
    "        Q[3,:] = 1\n",
    "        Q[6,:] = -1\n",
    "        #print(Q)\n",
    "    \n",
    "    # improve policy\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    m = np.argmax(Q,1)\n",
    "    for i in range(N_STATES):\n",
    "        policy[i,m[i]] = 1\n",
    "    \n",
    "    if t != epoch-1: \n",
    "        Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "        Q[3,:] = 1\n",
    "        Q[6,:] = -1\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1,3,-2,0]\n",
    "m = np.argmax(a)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[[ 1  5 -2  0]\n",
      " [ 1 -2  5  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,3,-2,0],[1,-2,3,0]])\n",
    "m = np.argmax(a,1)\n",
    "print(m)\n",
    "\n",
    "for i in range(2):\n",
    "    a[i,m[i]]=5\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial policy [0, 0, 0, 0, 0]\n",
      "State 1 : q_sa 85.0 q_best 0.0\n",
      "State 3 : q_sa 1000.75 q_best 0.0\n",
      "State 4 : q_sa 1.75 q_best 1.0\n",
      "Iterations: 1\n",
      "State 0 : q_sa 64.75 q_best 1.0\n",
      "State 2 : q_sa 850.5625 q_best 100.0\n",
      "State 3 : q_sa 1001.3125 q_best 1000.75\n",
      "State 4 : q_sa 2.3125 q_best 1.75\n",
      "Iterations: 2\n",
      "State 1 : q_sa 647.921875 q_best 85.0\n",
      "State 2 : q_sa 850.984375 q_best 850.5625\n",
      "State 3 : q_sa 1001.734375 q_best 1001.3125\n",
      "State 4 : q_sa 2.734375 q_best 2.3125\n",
      "Iterations: 3\n",
      "State 0 : q_sa 486.94140625 q_best 64.75\n",
      "State 1 : q_sa 648.23828125 q_best 647.921875\n",
      "State 2 : q_sa 851.30078125 q_best 850.984375\n",
      "State 3 : q_sa 1002.05078125 q_best 1001.734375\n",
      "State 4 : q_sa 3.05078125 q_best 2.734375\n",
      "Iterations: 4\n",
      "State 0 : q_sa 487.178710938 q_best 486.94140625\n",
      "State 1 : q_sa 648.475585938 q_best 648.23828125\n",
      "State 2 : q_sa 851.538085938 q_best 851.30078125\n",
      "State 3 : q_sa 1002.28808594 q_best 1002.05078125\n",
      "State 4 : q_sa 3.2880859375 q_best 3.05078125\n",
      "Iterations: 5\n",
      "State 0 : q_sa 487.356689453 q_best 487.178710938\n",
      "State 1 : q_sa 648.653564453 q_best 648.475585938\n",
      "State 2 : q_sa 851.716064453 q_best 851.538085938\n",
      "State 3 : q_sa 1002.46606445 q_best 1002.28808594\n",
      "State 4 : q_sa 3.46606445312 q_best 3.2880859375\n",
      "Iterations: 6\n",
      "State 0 : q_sa 487.49017334 q_best 487.356689453\n",
      "State 1 : q_sa 648.78704834 q_best 648.653564453\n",
      "State 2 : q_sa 851.84954834 q_best 851.716064453\n",
      "State 3 : q_sa 1002.59954834 q_best 1002.46606445\n",
      "State 4 : q_sa 3.59954833984 q_best 3.46606445312\n",
      "Iterations: 7\n",
      "State 0 : q_sa 487.590286255 q_best 487.49017334\n",
      "State 1 : q_sa 648.887161255 q_best 648.78704834\n",
      "State 2 : q_sa 851.949661255 q_best 851.84954834\n",
      "State 3 : q_sa 1002.69966125 q_best 1002.59954834\n",
      "State 4 : q_sa 3.69966125488 q_best 3.59954833984\n",
      "Iterations: 8\n",
      "State 0 : q_sa 487.665370941 q_best 487.590286255\n",
      "State 1 : q_sa 648.962245941 q_best 648.887161255\n",
      "State 2 : q_sa 852.024745941 q_best 851.949661255\n",
      "State 3 : q_sa 1002.77474594 q_best 1002.69966125\n",
      "State 4 : q_sa 3.77474594116 q_best 3.69966125488\n",
      "Iterations: 9\n",
      "State 0 : q_sa 487.721684456 q_best 487.665370941\n",
      "State 1 : q_sa 649.018559456 q_best 648.962245941\n",
      "State 2 : q_sa 852.081059456 q_best 852.024745941\n",
      "State 3 : q_sa 1002.83105946 q_best 1002.77474594\n",
      "State 4 : q_sa 3.83105945587 q_best 3.77474594116\n",
      "Iterations: 10\n",
      "State 0 : q_sa 487.763919592 q_best 487.721684456\n",
      "State 1 : q_sa 649.060794592 q_best 649.018559456\n",
      "State 2 : q_sa 852.123294592 q_best 852.081059456\n",
      "State 3 : q_sa 1002.87329459 q_best 1002.83105946\n",
      "State 4 : q_sa 3.8732945919 q_best 3.83105945587\n",
      "Iterations: 11\n",
      "State 0 : q_sa 487.795595944 q_best 487.763919592\n",
      "State 1 : q_sa 649.092470944 q_best 649.060794592\n",
      "State 2 : q_sa 852.154970944 q_best 852.123294592\n",
      "State 3 : q_sa 1002.90497094 q_best 1002.87329459\n",
      "State 4 : q_sa 3.90497094393 q_best 3.8732945919\n",
      "Iterations: 12\n",
      "State 0 : q_sa 487.819353208 q_best 487.795595944\n",
      "State 1 : q_sa 649.116228208 q_best 649.092470944\n",
      "State 2 : q_sa 852.178728208 q_best 852.154970944\n",
      "State 3 : q_sa 1002.92872821 q_best 1002.90497094\n",
      "State 4 : q_sa 3.92872820795 q_best 3.90497094393\n",
      "Iterations: 13\n",
      "State 0 : q_sa 487.837171156 q_best 487.819353208\n",
      "State 1 : q_sa 649.134046156 q_best 649.116228208\n",
      "State 2 : q_sa 852.196546156 q_best 852.178728208\n",
      "State 3 : q_sa 1002.94654616 q_best 1002.92872821\n",
      "State 4 : q_sa 3.94654615596 q_best 3.92872820795\n",
      "Iterations: 14\n",
      "State 0 : q_sa 487.850534617 q_best 487.837171156\n",
      "State 1 : q_sa 649.147409617 q_best 649.134046156\n",
      "State 2 : q_sa 852.209909617 q_best 852.196546156\n",
      "State 3 : q_sa 1002.95990962 q_best 1002.94654616\n",
      "State 4 : q_sa 3.95990961697 q_best 3.94654615596\n",
      "Iterations: 15\n",
      "State 0 : q_sa 487.860557213 q_best 487.850534617\n",
      "State 1 : q_sa 649.157432213 q_best 649.147409617\n",
      "State 2 : q_sa 852.219932213 q_best 852.209909617\n",
      "State 3 : q_sa 1002.96993221 q_best 1002.95990962\n",
      "State 4 : q_sa 3.96993221273 q_best 3.95990961697\n",
      "Iterations: 16\n",
      "State 0 : q_sa 487.86807416 q_best 487.860557213\n",
      "State 1 : q_sa 649.16494916 q_best 649.157432213\n",
      "State 2 : q_sa 852.22744916 q_best 852.219932213\n",
      "State 3 : q_sa 1002.97744916 q_best 1002.96993221\n",
      "State 4 : q_sa 3.97744915955 q_best 3.96993221273\n",
      "Iterations: 17\n",
      "State 0 : q_sa 487.87371187 q_best 487.86807416\n",
      "State 1 : q_sa 649.17058687 q_best 649.16494916\n",
      "State 2 : q_sa 852.23308687 q_best 852.22744916\n",
      "State 3 : q_sa 1002.98308687 q_best 1002.97744916\n",
      "State 4 : q_sa 3.98308686966 q_best 3.97744915955\n",
      "Iterations: 18\n",
      "State 0 : q_sa 487.877940152 q_best 487.87371187\n",
      "State 1 : q_sa 649.174815152 q_best 649.17058687\n",
      "State 2 : q_sa 852.237315152 q_best 852.23308687\n",
      "State 3 : q_sa 1002.98731515 q_best 1002.98308687\n",
      "State 4 : q_sa 3.98731515224 q_best 3.98308686966\n",
      "Iterations: 19\n",
      "State 0 : q_sa 487.881111364 q_best 487.877940152\n",
      "State 1 : q_sa 649.177986364 q_best 649.174815152\n",
      "State 2 : q_sa 852.240486364 q_best 852.237315152\n",
      "State 3 : q_sa 1002.99048636 q_best 1002.98731515\n",
      "State 4 : q_sa 3.99048636418 q_best 3.98731515224\n",
      "Iterations: 20\n",
      "State 0 : q_sa 487.883489773 q_best 487.881111364\n",
      "State 1 : q_sa 649.180364773 q_best 649.177986364\n",
      "State 2 : q_sa 852.242864773 q_best 852.240486364\n",
      "State 3 : q_sa 1002.99286477 q_best 1002.99048636\n",
      "State 4 : q_sa 3.99286477314 q_best 3.99048636418\n",
      "Iterations: 21\n",
      "State 0 : q_sa 487.88527358 q_best 487.883489773\n",
      "State 1 : q_sa 649.18214858 q_best 649.180364773\n",
      "State 2 : q_sa 852.24464858 q_best 852.242864773\n",
      "State 3 : q_sa 1002.99464858 q_best 1002.99286477\n",
      "State 4 : q_sa 3.99464857985 q_best 3.99286477314\n",
      "Iterations: 22\n",
      "State 0 : q_sa 487.886611435 q_best 487.88527358\n",
      "State 1 : q_sa 649.183486435 q_best 649.18214858\n",
      "State 2 : q_sa 852.245986435 q_best 852.24464858\n",
      "State 3 : q_sa 1002.99598643 q_best 1002.99464858\n",
      "State 4 : q_sa 3.99598643489 q_best 3.99464857985\n",
      "Iterations: 23\n",
      "State 0 : q_sa 487.887614826 q_best 487.886611435\n",
      "State 1 : q_sa 649.184489826 q_best 649.183486435\n",
      "State 2 : q_sa 852.246989826 q_best 852.245986435\n",
      "State 3 : q_sa 1002.99698983 q_best 1002.99598643\n",
      "State 4 : q_sa 3.99698982617 q_best 3.99598643489\n",
      "Iterations: 24\n",
      "State 0 : q_sa 487.88836737 q_best 487.887614826\n",
      "State 1 : q_sa 649.18524237 q_best 649.184489826\n",
      "State 2 : q_sa 852.24774237 q_best 852.246989826\n",
      "State 3 : q_sa 1002.99774237 q_best 1002.99698983\n",
      "State 4 : q_sa 3.99774236963 q_best 3.99698982617\n",
      "Iterations: 25\n",
      "State 0 : q_sa 487.888931777 q_best 487.88836737\n",
      "State 1 : q_sa 649.185806777 q_best 649.18524237\n",
      "State 2 : q_sa 852.248306777 q_best 852.24774237\n",
      "State 3 : q_sa 1002.99830678 q_best 1002.99774237\n",
      "State 4 : q_sa 3.99830677722 q_best 3.99774236963\n",
      "Iterations: 26\n",
      "State 0 : q_sa 487.889355083 q_best 487.888931777\n",
      "State 1 : q_sa 649.186230083 q_best 649.185806777\n",
      "State 2 : q_sa 852.248730083 q_best 852.248306777\n",
      "State 3 : q_sa 1002.99873008 q_best 1002.99830678\n",
      "State 4 : q_sa 3.99873008291 q_best 3.99830677722\n",
      "Iterations: 27\n",
      "State 0 : q_sa 487.889672562 q_best 487.889355083\n",
      "State 1 : q_sa 649.186547562 q_best 649.186230083\n",
      "State 2 : q_sa 852.249047562 q_best 852.248730083\n",
      "State 3 : q_sa 1002.99904756 q_best 1002.99873008\n",
      "State 4 : q_sa 3.99904756219 q_best 3.99873008291\n",
      "Iterations: 28\n",
      "State 0 : q_sa 487.889910672 q_best 487.889672562\n",
      "State 1 : q_sa 649.186785672 q_best 649.186547562\n",
      "State 2 : q_sa 852.249285672 q_best 852.249047562\n",
      "State 3 : q_sa 1002.99928567 q_best 1002.99904756\n",
      "State 4 : q_sa 3.99928567164 q_best 3.99904756219\n",
      "Iterations: 29\n",
      "State 0 : q_sa 487.890089254 q_best 487.889910672\n",
      "State 1 : q_sa 649.186964254 q_best 649.186785672\n",
      "State 2 : q_sa 852.249464254 q_best 852.249285672\n",
      "State 3 : q_sa 1002.99946425 q_best 1002.99928567\n",
      "State 4 : q_sa 3.99946425373 q_best 3.99928567164\n",
      "Iterations: 30\n",
      "State 0 : q_sa 487.89022319 q_best 487.890089254\n",
      "State 1 : q_sa 649.18709819 q_best 649.186964254\n",
      "State 2 : q_sa 852.24959819 q_best 852.249464254\n",
      "State 3 : q_sa 1002.99959819 q_best 1002.99946425\n",
      "State 4 : q_sa 3.9995981903 q_best 3.99946425373\n",
      "Iterations: 31\n",
      "State 0 : q_sa 487.890323643 q_best 487.89022319\n",
      "State 1 : q_sa 649.187198643 q_best 649.18709819\n",
      "State 2 : q_sa 852.249698643 q_best 852.24959819\n",
      "State 3 : q_sa 1002.99969864 q_best 1002.99959819\n",
      "State 4 : q_sa 3.99969864272 q_best 3.9995981903\n",
      "Iterations: 32\n",
      "State 0 : q_sa 487.890398982 q_best 487.890323643\n",
      "State 1 : q_sa 649.187273982 q_best 649.187198643\n",
      "State 2 : q_sa 852.249773982 q_best 852.249698643\n",
      "State 3 : q_sa 1002.99977398 q_best 1002.99969864\n",
      "State 4 : q_sa 3.99977398204 q_best 3.99969864272\n",
      "Iterations: 33\n",
      "State 0 : q_sa 487.890455487 q_best 487.890398982\n",
      "State 1 : q_sa 649.187330487 q_best 649.187273982\n",
      "State 2 : q_sa 852.249830487 q_best 852.249773982\n",
      "State 3 : q_sa 1002.99983049 q_best 1002.99977398\n",
      "State 4 : q_sa 3.99983048653 q_best 3.99977398204\n",
      "Iterations: 34\n",
      "State 0 : q_sa 487.890497865 q_best 487.890455487\n",
      "State 1 : q_sa 649.187372865 q_best 649.187330487\n",
      "State 2 : q_sa 852.249872865 q_best 852.249830487\n",
      "State 3 : q_sa 1002.99987286 q_best 1002.99983049\n",
      "State 4 : q_sa 3.9998728649 q_best 3.99983048653\n",
      "Iterations: 35\n",
      "State 0 : q_sa 487.890529649 q_best 487.890497865\n",
      "State 1 : q_sa 649.187404649 q_best 649.187372865\n",
      "State 2 : q_sa 852.249904649 q_best 852.249872865\n",
      "State 3 : q_sa 1002.99990465 q_best 1002.99987286\n",
      "State 4 : q_sa 3.99990464867 q_best 3.9998728649\n",
      "Iterations: 36\n",
      "State 0 : q_sa 487.890553487 q_best 487.890529649\n",
      "State 1 : q_sa 649.187428487 q_best 649.187404649\n",
      "State 2 : q_sa 852.249928487 q_best 852.249904649\n",
      "State 3 : q_sa 1002.99992849 q_best 1002.99990465\n",
      "State 4 : q_sa 3.99992848651 q_best 3.99990464867\n",
      "Iterations: 37\n",
      "State 0 : q_sa 487.890571365 q_best 487.890553487\n",
      "State 1 : q_sa 649.187446365 q_best 649.187428487\n",
      "State 2 : q_sa 852.249946365 q_best 852.249928487\n",
      "State 3 : q_sa 1002.99994636 q_best 1002.99992849\n",
      "State 4 : q_sa 3.99994636488 q_best 3.99992848651\n",
      "Iterations: 38\n",
      "State 0 : q_sa 487.890584774 q_best 487.890571365\n",
      "State 1 : q_sa 649.187459774 q_best 649.187446365\n",
      "State 2 : q_sa 852.249959774 q_best 852.249946365\n",
      "State 3 : q_sa 1002.99995977 q_best 1002.99994636\n",
      "State 4 : q_sa 3.99995977366 q_best 3.99994636488\n",
      "Iterations: 39\n",
      "State 0 : q_sa 487.89059483 q_best 487.890584774\n",
      "State 1 : q_sa 649.18746983 q_best 649.187459774\n",
      "State 2 : q_sa 852.24996983 q_best 852.249959774\n",
      "State 3 : q_sa 1002.99996983 q_best 1002.99995977\n",
      "State 4 : q_sa 3.99996983024 q_best 3.99995977366\n",
      "Iterations: 40\n",
      "State 0 : q_sa 487.890602373 q_best 487.89059483\n",
      "State 1 : q_sa 649.187477373 q_best 649.18746983\n",
      "State 2 : q_sa 852.249977373 q_best 852.24996983\n",
      "State 3 : q_sa 1002.99997737 q_best 1002.99996983\n",
      "State 4 : q_sa 3.99997737268 q_best 3.99996983024\n",
      "Iterations: 41\n",
      "State 0 : q_sa 487.89060803 q_best 487.890602373\n",
      "State 1 : q_sa 649.18748303 q_best 649.187477373\n",
      "State 2 : q_sa 852.24998303 q_best 852.249977373\n",
      "State 3 : q_sa 1002.99998303 q_best 1002.99997737\n",
      "State 4 : q_sa 3.99998302951 q_best 3.99997737268\n",
      "Iterations: 42\n",
      "State 0 : q_sa 487.890612272 q_best 487.89060803\n",
      "State 1 : q_sa 649.187487272 q_best 649.18748303\n",
      "State 2 : q_sa 852.249987272 q_best 852.24998303\n",
      "State 3 : q_sa 1002.99998727 q_best 1002.99998303\n",
      "State 4 : q_sa 3.99998727213 q_best 3.99998302951\n",
      "Iterations: 43\n",
      "State 0 : q_sa 487.890615454 q_best 487.890612272\n",
      "State 1 : q_sa 649.187490454 q_best 649.187487272\n",
      "State 2 : q_sa 852.249990454 q_best 852.249987272\n",
      "State 3 : q_sa 1002.99999045 q_best 1002.99998727\n",
      "State 4 : q_sa 3.9999904541 q_best 3.99998727213\n",
      "Iterations: 44\n",
      "State 0 : q_sa 487.890617841 q_best 487.890615454\n",
      "State 1 : q_sa 649.187492841 q_best 649.187490454\n",
      "State 2 : q_sa 852.249992841 q_best 852.249990454\n",
      "State 3 : q_sa 1002.99999284 q_best 1002.99999045\n",
      "State 4 : q_sa 3.99999284058 q_best 3.9999904541\n",
      "Iterations: 45\n",
      "State 0 : q_sa 487.89061963 q_best 487.890617841\n",
      "State 1 : q_sa 649.18749463 q_best 649.187492841\n",
      "State 2 : q_sa 852.24999463 q_best 852.249992841\n",
      "State 3 : q_sa 1002.99999463 q_best 1002.99999284\n",
      "State 4 : q_sa 3.99999463043 q_best 3.99999284058\n",
      "Iterations: 46\n",
      "State 0 : q_sa 487.890620973 q_best 487.89061963\n",
      "State 1 : q_sa 649.187495973 q_best 649.18749463\n",
      "State 2 : q_sa 852.249995973 q_best 852.24999463\n",
      "State 3 : q_sa 1002.99999597 q_best 1002.99999463\n",
      "State 4 : q_sa 3.99999597282 q_best 3.99999463043\n",
      "Iterations: 47\n",
      "State 0 : q_sa 487.89062198 q_best 487.890620973\n",
      "State 1 : q_sa 649.18749698 q_best 649.187495973\n",
      "State 2 : q_sa 852.24999698 q_best 852.249995973\n",
      "State 3 : q_sa 1002.99999698 q_best 1002.99999597\n",
      "State 4 : q_sa 3.99999697962 q_best 3.99999597282\n",
      "Iterations: 48\n",
      "State 0 : q_sa 487.890622735 q_best 487.89062198\n",
      "State 1 : q_sa 649.187497735 q_best 649.18749698\n",
      "State 2 : q_sa 852.249997735 q_best 852.24999698\n",
      "State 3 : q_sa 1002.99999773 q_best 1002.99999698\n",
      "State 4 : q_sa 3.99999773471 q_best 3.99999697962\n",
      "Iterations: 49\n",
      "State 0 : q_sa 487.890623301 q_best 487.890622735\n",
      "State 1 : q_sa 649.187498301 q_best 649.187497735\n",
      "State 2 : q_sa 852.249998301 q_best 852.249997735\n",
      "State 3 : q_sa 1002.9999983 q_best 1002.99999773\n",
      "State 4 : q_sa 3.99999830104 q_best 3.99999773471\n",
      "Iterations: 50\n",
      "State 0 : q_sa 487.890623726 q_best 487.890623301\n",
      "State 1 : q_sa 649.187498726 q_best 649.187498301\n",
      "State 2 : q_sa 852.249998726 q_best 852.249998301\n",
      "State 3 : q_sa 1002.99999873 q_best 1002.9999983\n",
      "State 4 : q_sa 3.99999872578 q_best 3.99999830104\n",
      "Iterations: 51\n",
      "State 0 : q_sa 487.890624044 q_best 487.890623726\n",
      "State 1 : q_sa 649.187499044 q_best 649.187498726\n",
      "State 2 : q_sa 852.249999044 q_best 852.249998726\n",
      "State 3 : q_sa 1002.99999904 q_best 1002.99999873\n",
      "State 4 : q_sa 3.99999904433 q_best 3.99999872578\n",
      "Iterations: 52\n",
      "State 0 : q_sa 487.890624283 q_best 487.890624044\n",
      "State 1 : q_sa 649.187499283 q_best 649.187499044\n",
      "State 2 : q_sa 852.249999283 q_best 852.249999044\n",
      "State 3 : q_sa 1002.99999928 q_best 1002.99999904\n",
      "State 4 : q_sa 3.99999928325 q_best 3.99999904433\n",
      "Iterations: 53\n",
      "State 0 : q_sa 487.890624462 q_best 487.890624283\n",
      "State 1 : q_sa 649.187499462 q_best 649.187499283\n",
      "State 2 : q_sa 852.249999462 q_best 852.249999283\n",
      "State 3 : q_sa 1002.99999946 q_best 1002.99999928\n",
      "State 4 : q_sa 3.99999946244 q_best 3.99999928325\n",
      "Iterations: 54\n",
      "State 0 : q_sa 487.890624597 q_best 487.890624462\n",
      "State 1 : q_sa 649.187499597 q_best 649.187499462\n",
      "State 2 : q_sa 852.249999597 q_best 852.249999462\n",
      "State 3 : q_sa 1002.9999996 q_best 1002.99999946\n",
      "State 4 : q_sa 3.99999959683 q_best 3.99999946244\n",
      "Iterations: 55\n",
      "State 0 : q_sa 487.890624698 q_best 487.890624597\n",
      "State 1 : q_sa 649.187499698 q_best 649.187499597\n",
      "State 2 : q_sa 852.249999698 q_best 852.249999597\n",
      "State 3 : q_sa 1002.9999997 q_best 1002.9999996\n",
      "State 4 : q_sa 3.99999969762 q_best 3.99999959683\n",
      "Iterations: 56\n",
      "State 0 : q_sa 487.890624773 q_best 487.890624698\n",
      "State 1 : q_sa 649.187499773 q_best 649.187499698\n",
      "State 2 : q_sa 852.249999773 q_best 852.249999698\n",
      "State 3 : q_sa 1002.99999977 q_best 1002.9999997\n",
      "State 4 : q_sa 3.99999977322 q_best 3.99999969762\n",
      "Iterations: 57\n",
      "State 0 : q_sa 487.89062483 q_best 487.890624773\n",
      "State 1 : q_sa 649.18749983 q_best 649.187499773\n",
      "State 2 : q_sa 852.24999983 q_best 852.249999773\n",
      "State 3 : q_sa 1002.99999983 q_best 1002.99999977\n",
      "State 4 : q_sa 3.99999982991 q_best 3.99999977322\n",
      "Iterations: 58\n",
      "State 0 : q_sa 487.890624872 q_best 487.89062483\n",
      "State 1 : q_sa 649.187499872 q_best 649.18749983\n",
      "State 2 : q_sa 852.249999872 q_best 852.24999983\n",
      "State 3 : q_sa 1002.99999987 q_best 1002.99999983\n",
      "State 4 : q_sa 3.99999987243 q_best 3.99999982991\n",
      "Iterations: 59\n",
      "State 0 : q_sa 487.890624904 q_best 487.890624872\n",
      "State 1 : q_sa 649.187499904 q_best 649.187499872\n",
      "State 2 : q_sa 852.249999904 q_best 852.249999872\n",
      "State 3 : q_sa 1002.9999999 q_best 1002.99999987\n",
      "State 4 : q_sa 3.99999990433 q_best 3.99999987243\n",
      "Iterations: 60\n",
      "State 0 : q_sa 487.890624928 q_best 487.890624904\n",
      "State 1 : q_sa 649.187499928 q_best 649.187499904\n",
      "State 2 : q_sa 852.249999928 q_best 852.249999904\n",
      "State 3 : q_sa 1002.99999993 q_best 1002.9999999\n",
      "State 4 : q_sa 3.99999992824 q_best 3.99999990433\n",
      "Iterations: 61\n",
      "State 0 : q_sa 487.890624946 q_best 487.890624928\n",
      "State 1 : q_sa 649.187499946 q_best 649.187499928\n",
      "State 2 : q_sa 852.249999946 q_best 852.249999928\n",
      "State 3 : q_sa 1002.99999995 q_best 1002.99999993\n",
      "State 4 : q_sa 3.99999994618 q_best 3.99999992824\n",
      "Iterations: 62\n",
      "State 0 : q_sa 487.89062496 q_best 487.890624946\n",
      "State 1 : q_sa 649.18749996 q_best 649.187499946\n",
      "State 2 : q_sa 852.24999996 q_best 852.249999946\n",
      "State 3 : q_sa 1002.99999996 q_best 1002.99999995\n",
      "State 4 : q_sa 3.99999995964 q_best 3.99999994618\n",
      "Iterations: 63\n",
      "State 0 : q_sa 487.89062497 q_best 487.89062496\n",
      "State 1 : q_sa 649.18749997 q_best 649.18749996\n",
      "State 2 : q_sa 852.24999997 q_best 852.24999996\n",
      "State 3 : q_sa 1002.99999997 q_best 1002.99999996\n",
      "State 4 : q_sa 3.99999996973 q_best 3.99999995964\n",
      "Iterations: 64\n",
      "State 0 : q_sa 487.890624977 q_best 487.89062497\n",
      "State 1 : q_sa 649.187499977 q_best 649.18749997\n",
      "State 2 : q_sa 852.249999977 q_best 852.24999997\n",
      "State 3 : q_sa 1002.99999998 q_best 1002.99999997\n",
      "State 4 : q_sa 3.9999999773 q_best 3.99999996973\n",
      "Iterations: 65\n",
      "State 0 : q_sa 487.890624983 q_best 487.890624977\n",
      "State 1 : q_sa 649.187499983 q_best 649.187499977\n",
      "State 2 : q_sa 852.249999983 q_best 852.249999977\n",
      "State 3 : q_sa 1002.99999998 q_best 1002.99999998\n",
      "State 4 : q_sa 3.99999998297 q_best 3.9999999773\n",
      "Iterations: 66\n",
      "State 0 : q_sa 487.890624987 q_best 487.890624983\n",
      "State 1 : q_sa 649.187499987 q_best 649.187499983\n",
      "State 2 : q_sa 852.249999987 q_best 852.249999983\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999998\n",
      "State 4 : q_sa 3.99999998723 q_best 3.99999998297\n",
      "Iterations: 67\n",
      "State 0 : q_sa 487.89062499 q_best 487.890624987\n",
      "State 1 : q_sa 649.18749999 q_best 649.187499987\n",
      "State 2 : q_sa 852.24999999 q_best 852.249999987\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999042 q_best 3.99999998723\n",
      "Iterations: 68\n",
      "State 0 : q_sa 487.890624993 q_best 487.89062499\n",
      "State 1 : q_sa 649.187499993 q_best 649.18749999\n",
      "State 2 : q_sa 852.249999993 q_best 852.24999999\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999282 q_best 3.99999999042\n",
      "Iterations: 69\n",
      "State 0 : q_sa 487.890624995 q_best 487.890624993\n",
      "State 1 : q_sa 649.187499995 q_best 649.187499993\n",
      "State 2 : q_sa 852.249999995 q_best 852.249999993\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999461 q_best 3.99999999282\n",
      "Iterations: 70\n",
      "State 0 : q_sa 487.890624996 q_best 487.890624995\n",
      "State 1 : q_sa 649.187499996 q_best 649.187499995\n",
      "State 2 : q_sa 852.249999996 q_best 852.249999995\n",
      "State 3 : q_sa 1003.0 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999596 q_best 3.99999999461\n",
      "Iterations: 71\n",
      "State 0 : q_sa 487.890624997 q_best 487.890624996\n",
      "State 1 : q_sa 649.187499997 q_best 649.187499996\n",
      "State 2 : q_sa 852.249999997 q_best 852.249999996\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999697 q_best 3.99999999596\n",
      "Iterations: 72\n",
      "State 0 : q_sa 487.890624998 q_best 487.890624997\n",
      "State 1 : q_sa 649.187499998 q_best 649.187499997\n",
      "State 2 : q_sa 852.249999998 q_best 852.249999997\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999773 q_best 3.99999999697\n",
      "Iterations: 73\n",
      "State 0 : q_sa 487.890624998 q_best 487.890624998\n",
      "State 1 : q_sa 649.187499998 q_best 649.187499998\n",
      "State 2 : q_sa 852.249999998 q_best 852.249999998\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999983 q_best 3.99999999773\n",
      "Iterations: 74\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624998\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499998\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999998\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999872 q_best 3.9999999983\n",
      "Iterations: 75\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624999\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499999\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999904 q_best 3.99999999872\n",
      "Iterations: 76\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624999\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499999\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999928 q_best 3.99999999904\n",
      "Iterations: 77\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624999\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499999\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999946 q_best 3.99999999928\n",
      "Iterations: 78\n",
      "State 0 : q_sa 487.890625 q_best 487.890624999\n",
      "State 1 : q_sa 649.1875 q_best 649.187499999\n",
      "State 2 : q_sa 852.25 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999996 q_best 3.99999999946\n",
      "Iterations: 79\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999997 q_best 3.9999999996\n",
      "Iterations: 80\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999977 q_best 3.9999999997\n",
      "Iterations: 81\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999983 q_best 3.99999999977\n",
      "Iterations: 82\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999987 q_best 3.99999999983\n",
      "Iterations: 83\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999999 q_best 3.99999999987\n",
      "Iterations: 84\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999993 q_best 3.9999999999\n",
      "Iterations: 85\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999995 q_best 3.99999999993\n",
      "Iterations: 86\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999996 q_best 3.99999999995\n",
      "Iterations: 87\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999997 q_best 3.99999999996\n",
      "Iterations: 88\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999998 q_best 3.99999999997\n",
      "Iterations: 89\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999998 q_best 3.99999999998\n",
      "Iterations: 90\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999998\n",
      "Iterations: 91\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999999\n",
      "Iterations: 92\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999999\n",
      "Iterations: 93\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999999\n",
      "Iterations: 94\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 3.99999999999\n",
      "Iterations: 95\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 96\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 97\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 98\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 99\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 100\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 101\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 102\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 103\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 104\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 105\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 106\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 107\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 108\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 109\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 110\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 111\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 112\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 113\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 114\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 115\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 116\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 117\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 118\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 119\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 120\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 121\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 122\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 123\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iterations: 124\n",
      "Iterations: 125\n",
      "Final policy\n",
      "[0, 1, 0, 1, 0]\n",
      "[  487.890625   649.1875     852.25      1003.           4.      ]\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration in Python\n",
    "# https://gist.github.com/tuxdna/7e29dd37300e308a80fc1559c343c545\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "1: Procedure Policy_Iteration(S,A,P,R)\n",
    "2:           Inputs\n",
    "3:                     S is the set of all states\n",
    "4:                     A is the set of all actions\n",
    "5:                     P is state transition function specifying P(s'|s,a)\n",
    "6:                     R is a reward function R(s,a,s')\n",
    "7:           Output\n",
    "8:                     optimal policy \n",
    "9:           Local\n",
    "10:                     action array [S]\n",
    "11:                     Boolean variable noChange\n",
    "12:                     real array V[S]\n",
    "13:           set  arbitrarily\n",
    "14:           repeat\n",
    "15:                     noChange true\n",
    "16:                     Solve V[s] = s'S P(s'|s,[s])(R(s,a,s')+V[s'])\n",
    "17:                     for each sS do\n",
    "18:                               Let QBest=V[s]\n",
    "19:                               for each a A do\n",
    "20:                                         Let Qsa=s'S P(s'|s,a)(R(s,a,s')+V[s'])\n",
    "21:                                         if (Qsa > QBest) then\n",
    "22:                                                   [s]a\n",
    "23:                                                   QBest Qsa\n",
    "24:                                                   noChange false\n",
    "25:           until noChange\n",
    "26:           return \n",
    "\"\"\"\n",
    "\n",
    "states = [0,1,2,3,4]\n",
    "actions = [0,1]\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # transition probability\n",
    "R = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # rewards\n",
    "\n",
    "P[0,0,1] = 1.0\n",
    "P[1,1,2] = 1.0\n",
    "P[2,0,3] = 1.0\n",
    "P[3,1,4] = 1.0\n",
    "P[4,0,4] = 1.0\n",
    "\n",
    "\n",
    "R[0,0,1] = 1\n",
    "R[1,1,2] = 10\n",
    "R[2,0,3] = 100\n",
    "R[3,1,4] = 1000\n",
    "R[4,0,4] = 1.0\n",
    "\n",
    "\n",
    "gamma = 0.75\n",
    "\n",
    "# initialize policy and value arbitrarily\n",
    "policy = [0 for s in range(N_STATES)]\n",
    "V = np.zeros(N_STATES)\n",
    "\n",
    "print(\"Initial policy\", policy)\n",
    "# print(V)\n",
    "# print(P)\n",
    "# print(R)\n",
    "\n",
    "is_value_changed = True\n",
    "iterations = 0\n",
    "while is_value_changed:\n",
    "    is_value_changed = False\n",
    "    iterations += 1\n",
    "    # run value iteration for each state\n",
    "    for s in range(N_STATES):\n",
    "        V[s] = sum([P[s,policy[s],s1] * (R[s,policy[s],s1] + gamma*V[s1]) for s1 in range(N_STATES)])\n",
    "        # print(\"Run for state\", s)\n",
    "\n",
    "    for s in range(N_STATES):\n",
    "        q_best = V[s]\n",
    "        # print(\"State\", s, \"q_best\", q_best)\n",
    "        for a in range(N_ACTIONS):\n",
    "            q_sa = sum([P[s, a, s1] * (R[s, a, s1] + gamma * V[s1]) for s1 in range(N_STATES)])\n",
    "            if q_sa > q_best:\n",
    "                print(\"State\", s, \": q_sa\", q_sa, \"q_best\", q_best)\n",
    "                policy[s] = a\n",
    "                q_best = q_sa\n",
    "                is_value_changed = True\n",
    "\n",
    "    print(\"Iterations:\", iterations)\n",
    "    # print(\"Policy now\", policy)\n",
    "\n",
    "print(\"Final policy\")\n",
    "print(policy)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (utils.py, line 154)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/sungchul/Dropbox/Jupyter_Notebook/Machine_Learning/utils.py\"\u001b[0;36m, line \u001b[0;32m154\u001b[0m\n\u001b[0;31m    __xor__ = symmetric_difference\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# AIMA Python file: mdp.py\n",
    "# http://aima.cs.berkeley.edu/python/mdp.html\n",
    "\n",
    "\"\"\"Markov Decision Processes (Chapter 17)\n",
    "\n",
    "First we define an MDP, and the special case of a GridMDP, in which\n",
    "states are laid out in a 2-dimensional grid.  We also represent a policy\n",
    "as a dictionary of {state:action} pairs, and a Utility function as a\n",
    "dictionary of {state:number} pairs.  We then define the value_iteration\n",
    "and policy_iteration algorithms.\"\"\"\n",
    "\n",
    "from utils import *\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text.  Instead of T(s, a, s') being  probability number for each\n",
    "    state/action/state triplet, we instead have T(s, a) return a list of (p, s')\n",
    "    pairs.  We also keep track of the possible states, terminal states, and\n",
    "    actions for each state. [page 615]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, gamma=.9):\n",
    "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
    "               gamma=gamma, states=set(), reward={})\n",
    "\n",
    "    def R(self, state):\n",
    "        \"Return a numeric reward for this state.\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(state, action):\n",
    "        \"\"\"Transition model.  From a state and an action, return a list\n",
    "        of (result-state, probability) pairs.\"\"\"\n",
    "        abstract\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state).  Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse() ## because we want row 0 on bottom, not on top\n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action == None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, turn_right(action))),\n",
    "                    (0.1, self.go(state, turn_left(action)))]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        \"Return the state that results from going in this direction.\"\n",
    "        state1 = vector_add(state, direction)\n",
    "        return if_(state1 in self.states, state1, state)\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "        return list(reversed([[mapping.get((x,y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0):'>', (0, 1):'^', (-1, 0):'<', (0, -1):'v', None: '.'}\n",
    "        return self.to_grid(dict([(s, chars[a]) for (s, a) in policy.items()]))\n",
    "\n",
    "\n",
    "# Fig[17,1] = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "#                      [-0.04, None,  -0.04, -1],\n",
    "#                      [-0.04, -0.04, -0.04, -0.04]],\n",
    "#                     terminals=[(3, 2), (3, 1)])\n",
    "\n",
    "\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    \"Solving an MDP by value iteration. [Fig. 17.4]\"\n",
    "    U1 = dict([(s, 0) for s in mdp.states])\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "             return U\n",
    "\n",
    "def best_policy(mdp, U):\n",
    "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
    "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        pi[s] = argmax(mdp.actions(s), lambda a:expected_utility(a, s, U, mdp))\n",
    "    return pi\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    \"The expected utility of doing a in state s, according to the MDP and U.\"\n",
    "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])\n",
    "\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    \"Solve an MDP by policy iteration [Fig. 17.7]\"\n",
    "    U = dict([(s, 0) for s in mdp.states])\n",
    "    pi = dict([(s, random.choice(mdp.actions(s))) for s in mdp.states])\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = argmax(mdp.actions(s), lambda a: expected_utility(a,s,U,mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi\n",
    "\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n",
    "    utility, using an approximation (modified policy iteration).\"\"\"\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum([p * U[s] for (p, s1) in T(s, pi[s])])\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
