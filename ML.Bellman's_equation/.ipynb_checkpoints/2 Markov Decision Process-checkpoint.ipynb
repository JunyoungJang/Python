{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/probabilistic+black+friday.jpeg\"/>\n",
    "\n",
    "[Markov Chains - The Black Friday Puzzle](https://www.countbayesie.com/blog/2015/11/21/the-black-friday-puzzle-understanding-markov-chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP (Markov Decision Process)\n",
    "\n",
    "<img src=\"img/Markov Decision Process 2.png\"/>\n",
    "\n",
    "# Action-value function $q_\\pi(s,a)$ and state-value function $v_\\pi(s)$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&E_\\pi(G_t|S_t=s,A_t=a)\\nonumber\\\\\n",
    "\\\\\n",
    "v_\\pi(s)&=&E_\\pi(G_t|S_t=s)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<tr>\n",
    "<td> <img src=\"img/Bellman's expectation equation 1.png\"/> </td>\n",
    "<td> <img src=\"img/Bellman's expectation equation 2.png\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman's expectation equation for $v_\\pi$ and $q_\\pi$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)q_\\pi(s,a)\\nonumber\\\\\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimal action-value function, state-value function, policy \n",
    "\n",
    "<img src=\"img/Optimal Policy 1.png\"/>\n",
    "\n",
    "<img src=\"img/Optimal Policy 3.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"img/Bellman's optimality equation 1.png\"/> </td>\n",
    "<td> <img src=\"img/Bellman's optimality equation 2.png\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman optimality equation for $v_{*}$ and $q_{*}$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}q_*(s,a)\\nonumber\\\\\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\max_{a'}q_*(s',a')\\right)\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration\n",
    "\n",
    "- Initialize $v_*(s)=0$ for all $s$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ (synchronous or asynchronous) update $q_*$ and $v_*$ using Bellman's optimality equation: \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}q_*(s,a)\\nonumber\\\\\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\max_{a'}q_*(s',a')\\right)\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### [$Q$-learning](https://en.wikipedia.org/wiki/Q-learning)\n",
    "\n",
    "<img src=\"img/Q-learning algorithm.png\"/>\n",
    "\n",
    "### [SARSA](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action)\n",
    "\n",
    "<img src=\"img/SARSA algorithm.png\"/>\n",
    "\n",
    "### [Temporal difference learning](https://en.wikipedia.org/wiki/Temporal_difference_learning)\n",
    "\n",
    "<img src=\"img/Temporal difference learning algorithm.png\"/>\n",
    "\n",
    "### DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration\n",
    "\n",
    "- Initialize $\\pi$ randomly.\n",
    "\n",
    "- Repeat\n",
    "\n",
    "    Update $q_\\pi$ and $v_\\pi$ by solving Bellman's expectation equation.\n",
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)q_\\pi(s,a)\\nonumber\\\\\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "    \n",
    "    Update $\\pi$ by solving\n",
    "\n",
    "$$\n",
    "\\pi(s)=\\mbox{argmax}_{a}q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "\n",
    "### A3C\n",
    "\n",
    "### TRPO\n",
    "\n",
    "### ACKTR\n",
    "\n",
    "### DDPG\n",
    "\n",
    "### Reinforcement learning\n",
    "\n",
    "### GAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: A\n",
      "State is now: B\n",
      "New State: C\n",
      "New State: A\n",
      "New State: C\n",
      "New State: A\n",
      "New State: C\n",
      "New State: B\n",
      "New State: C\n",
      "New State: A\n",
      "New State: B\n",
      "New State: C\n"
     ]
    }
   ],
   "source": [
    "# Markov Chains in Python\n",
    "# http://charlesfranzen.com/posts/markov-chains-in-python/\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Markov(object):\n",
    "\n",
    "    def __init__(self, state_dict):\n",
    "        self.state_dict = state_dict\n",
    "        self.state = list(self.state_dict.keys())[0]\n",
    "\n",
    "    def check_state(self):\n",
    "        print('Current State: %s' % (self.state))\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        print('State is now: %s' % (self.state))\n",
    "\n",
    "    def next_state(self):\n",
    "        A = self.state_dict[self.state]\n",
    "        self.state = np.random.choice(a=list(A[0]), p=list(A[1]))\n",
    "        print('New State: %s' % (self.state))\n",
    "\n",
    "        \n",
    "state_dict = {'A': np.array([['A', 'B', 'C'],\n",
    "                             [.2, .4, .4]]),\n",
    "              'B': np.array([['A', 'C'],\n",
    "                             [.4, .6]]),\n",
    "              'C': np.array([['A', 'B'],\n",
    "                             [.6, .4]])}\n",
    "\n",
    "diagram_a = Markov(state_dict)\n",
    "diagram_a.check_state()\n",
    "\n",
    "diagram_a.set_state('B')\n",
    "\n",
    "for _ in range(10):\n",
    "    diagram_a.next_state()\n",
    "    \n",
    "# Exercise.\n",
    "# Modify the code so that we have rewards in addition.\n",
    "\n",
    "# Exercise.\n",
    "# Modify the code so that we have actions and rewards in addition.\n",
    "\n",
    "# Exercise.\n",
    "# Modify the code so that we have discount, actions, and rewards in addition."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
