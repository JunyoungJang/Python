{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bellman's equation\n",
    " \n",
    "Sungchul Lee  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/2007-164-water-policy.jpg\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "- When you are at state $s$, there are many actions you can choose.\n",
    "\n",
    "- Policy descibe how you choose your action.\n",
    "\n",
    "http://www.inkcinct.com.au/web-pages/cartoons/past/2007/2007-164-water-policy.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy in Andrew Ng's Lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2016-12-16+15.11.27.png\" width=\"80%\" height=\"10%\"></div>\n",
    "\n",
    "http://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/56381d00e4b05b1abc31cd96/58546ed09de4bb1925de9469/1494102176399/Screenshot+2016-12-16+15.11.27.png?format=1000w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bad policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Downarrow$|H|$\\Rightarrow$|-1|\n",
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Uparrow$|$\\Uparrow$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,0,1]\n",
    "policy[5,:] = [0,1,0,0]\n",
    "policy[6,:] = [0,1,0,0]\n",
    "policy[7,:] = [0,1,0,0]\n",
    "policy[8,:] = [0,1,0,0]\n",
    "policy[9,:] = [0,0,1,0]\n",
    "policy[10,:] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|$\\Rightarrow$|$\\Rightarrow$|$\\Rightarrow$|1|\n",
    "|------|------|------|------|\n",
    "|$\\Uparrow$|H|$\\Uparrow$|-1|\n",
    "|$\\Uparrow$|$\\Leftarrow$|$\\Leftarrow$|$\\Leftarrow$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,1,0]\n",
    "policy[5,:] = [0,0,1,0]\n",
    "policy[6,:] = [0,0,1,0]\n",
    "policy[7,:] = [0,0,1,0]\n",
    "policy[8,:] = [1,0,0,0]\n",
    "policy[9,:] = [1,0,0,0]\n",
    "policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Policy in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# policy\n",
    "if False: # bad policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif False: # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif True: # optimal policy \n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value function and Action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/Q-function.png\" width=\"70%\" height=\"10%\"></div>\n",
    "\n",
    "https://www.youtube.com/watch?v=Vd-gmo-qO5E&index=4&list=PLlMkM4tgfjnKsCWav-Z2F-MMFRx-2gMGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mbox{Value function}&&\\quad v_\\pi(s)&=&E_\\pi(G_t|S_t=s)\\\\\n",
    "\\mbox{Action-value function or Q function}&&\\quad q_\\pi(s,a)&=&E_\\pi(G_t|S_t=s,A_t=a)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "G_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\cdots++\\gamma^{T-t} R_{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/nature14236-sf2.jpg\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "https://images.nature.com/full/nature-assets/nature/journal/v518/n7540/images/nature14236-sf2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman's expectation equation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{eqnarray*}\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)q_\\pi(s,a)\\nonumber\\\\\n",
    "q_\\pi(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right)\\nonumber\\\\\n",
    "v_\\pi(s)&=&\\sum_{a}\\pi(a|s)\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_\\pi(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.43.10 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.43.24 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.43.35 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.43.46 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal policy, value function, and action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"img/cs188_mdp_policy_methods.png\" width=\"80%\"></div>\n",
    "\n",
    "https://github.com/mebusy/notes/blob/master/dev_notes/AI_CS188_MDP.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/Optimal Policy 1.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/Optimal Policy 3.png\"/>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman's optimality equation \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{eqnarray*}\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}q_*(s,a)\\nonumber\\\\\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\max_{a'}q_*(s',a')\\right)\\nonumber\\\\\n",
    "v_*(s)&=&\\max_{a}\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.43.59 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.44.10 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.44.18 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.44.24 PM.png\" width=\"100%\" height=\"10%\"></div>\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
